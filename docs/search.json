[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DATA / STAT 334",
    "section": "",
    "text": "Syllabus and Course Information"
  },
  {
    "objectID": "index.html#general-information",
    "href": "index.html#general-information",
    "title": "DATA / STAT 334",
    "section": "General Information",
    "text": "General Information\n\nInstructor Information\n\nProfessor: Matt Higham\nOffice: Bewkes 123\nEmail: mhigham@stlawu.edu\nOffice Hours:\n\nOffice Hours: 15 minute slots bookable at my calendly page.\n\nSections:\n\nMW 8:50 - 10:20\n\n\n\n\nCourse Materials\n\nlaptop capable of installing R, R Studio, and Git.\nour course website will be the primary source of materials. You can bookmark the site for easy access throughout the semester.\nwe will also use various online resources throughout the semester\n\nwe will occasionally use these course materials from my DATA/STAT 234 course  as a reference.\nwe will use this data visualization book by Kieran Healy a couple of times.\nhttps://github.com/highamm/dataviz_notes is a GitHub repository that will contain code from class."
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "DATA / STAT 334",
    "section": "Course Information",
    "text": "Course Information\nWelcome to DATA/STAT 334: Data Visualization! In this course, we will expand upon DATA/STAT 234 and STAT 213 to focus more on visualizing data and visualizing models. In the 9-10 weeks of the course, we will build a foundation for a more broad, conceptual understanding of data viz and its usefulness. In the final 4-5 weeks of the course, you will focus in-depth on visualizing one or more data sets in an extensive project.\n\nGeneral Course Outcomes\n\nExplain why it is useful to visualize data in a variety of contexts.\nExplain what makes a data graphic “good” and what makes a data graphic “poor.”\nDiscuss the ethics of various data visualizations.\nUse R and R Studio to construct various data visualizations for a variety of data sets.\nUse visualization to convey meaning the meaning of regression models to a general audience.\nUse Git and GitHub for version control and as a way to easily share with others your code and project.\nCreate a larger-scale final project that showcases an in-depth visual analysis of one or more data sets.\n\n\n\nUse of RStudio and GitHub\nWe will use the statistical software R and the R Studio IDE throughout the course for a few reasons:\n\nR and RStudio are both free to use.\nOne pre-requisite for this course is DATA/STAT 234, meaning that everyone in this course has some background already with using R.\nR Studio has a nice Git interface.\n\nIn addition to R Studio, we will use Git and GitHub for version control. GitHub will also be the location where you can obtain materials from missing class link to GitHub repository . We will discuss what these are and why they are useful more in class."
  },
  {
    "objectID": "index.html#how-you-will-be-assessed",
    "href": "index.html#how-you-will-be-assessed",
    "title": "DATA / STAT 334",
    "section": "How You Will Be Assessed",
    "text": "How You Will Be Assessed\nThere are 1000 total points that can be earned in this course. The primary purpose of the first two-thirds of the course is to give you breadth of knowledge in the topic of data visualization while the primary purpose for the last third of the course is to give you an opportunity to work on a visualization project of interest in sufficient depth. The specific components that you will be assessed on are described next.\n\n\nExercises and Participation\nThere will be sets of “Exercises”, each worth between 5 and 10 points for a total of 200 points. What these points will come from include:\n\nclass prep materials that consist of completing some of our course materials (in the sections called “Class Prep”) before class.\nevaluation of your participation in class a few times throughout the semester\nFeedback rubrics on final projects for other students in the class and/or GitHub pushes on your own final project (completed in the back third of semester).\n\nCollaboration is permitted on all exercise sets.\n\n\n\nBlog\nDuring the first part of the semester, you will write 3 blog posts on a site that you will construct with Quarto and publish this blog through GitHub pages. Each blog post is worth 30 points, and there will be an additional 10 points associated with the third blog post for a total of 100 points.\n\n\nAssessments\nThroughout the semester, there will be 5 assessments, each worth 100 points. Each assessment will consist of\n\na take-home component (worth approximately 10 points)\na handwritten in-class component (worth anywhere from 10 to 80 points).\na coding in-class component (worth anywhere from 10 to 80 points).\n\n\n\nFinal Project\nFinally, there are 200 points for the final project. These points will be split amongst a few different components of the final project: details will be provided later in the semester.\n\n\nPoint Breakdown\nTherefore, based on the information above, the point breakdown for the course is:\n\n200 points for exercises, participation, GitHub pushes, and feedback rubrics.\n100 points for blog posts.\n500 points for assessments.\n200 points for the final project.\n\nPoints add up to 1000 so your grade at the end of the semester will be the number of points you’ve earned across all categories divided by 1000."
  },
  {
    "objectID": "index.html#grading-scale",
    "href": "index.html#grading-scale",
    "title": "DATA / STAT 334",
    "section": "Grading Scale",
    "text": "Grading Scale\nThe following is a rough grading scale. I reserve the right to make any changes to the scale if necessary.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\n4.0\n3.75\n3.5\n3.25\n3.0\n2.75\n2.5\n2.25\n2.0\n1.75\n1.5\n1.25\n1.0\n0.0\n\n\n\n\nPoints\n950-1000\n920-949\n890-919\n860-889\n830-859\n810-829\n770-809\n750-769\n720-749\n700-719\n670-699\n640-669\n600-639\n0-599"
  },
  {
    "objectID": "index.html#collaboration-diversity-accessibility-and-academic-integrity",
    "href": "index.html#collaboration-diversity-accessibility-and-academic-integrity",
    "title": "DATA / STAT 334",
    "section": "Collaboration, Diversity, Accessibility, and Academic Integrity",
    "text": "Collaboration, Diversity, Accessibility, and Academic Integrity\n\nRules for Collaboration\nCollaboration with your classmates on the exercises is encouraged, but you must follow these guidelines:\n\nyou must state the name(s) of who you collaborated with at the top of each exercise set.\nall work must be your own. This means that you should never send someone your code via email or let someone directly type code off of your screen. Instead, you can talk about strategies for solving problems and help or ask someone about a coding error.\nyou may use the Internet and StackExchange, but you also should not copy paste code directly from the website, without citing that you did so.\nuse of generative AI (like Chat GPT) is not permitted on any of the assessments at all. If you use AI for help on something in your blog post or final project, you should include the search prompt and what you are using.\n\n\n\n\n\nDiversity Statement\nDiversity encompasses differences in age, colour, ethnicity, national origin, gender, physical or mental ability, religion, socioeconomic background, veteran status, sexual orientation, and marginalized groups. The interaction of different human characteristics brings about a positive learning environment. Diversity is both respected and valued in this classroom.\n\n\n\n\nAccessibility Statement\nYour experience in this class is important to me. It is the policy and practice of St. Lawrence University to create inclusive and accessible learning environments consistent with federal and state law. If you have already established accommodations with the Student Accessibility Services Office, please meet with them to activate your accommodations so we can discuss how they will be implemented in this course. If you have not yet established services through the Student Accessibility Services Office but have a temporary health condition or permanent disability that requires accommodations (conditions include but are not limited to; mental health, attention-related, learning, vision, hearing, physical or health impacts), please contact the Student Accessibility Services Office directly to set up a meeting to discuss establishing with their office. The Student Accessibility Services Office will work with you on the interactive process that establishes reasonable accommodations.\nColor-Vision Deficiency: If you are Color-Vision Deficient, the Student Accessibility Services office has on loan glasses for students who are color vision deficient. Please contact the office to make an appointment.\nFor more specific information about setting up an appointment with Student Accessibility Services please see the listed options below:\n\nTelephone: 315.229.5537\nEmail: studentaccessibility@stlawu.edu\n\nFor further information about Student Accessibility Services you can check the website at: https://www.stlawu.edu/student-accessibility-services\n\n\n\n\nAcademic Dishonesty\nAcademic dishonesty will not be tolerated. Any specific policies for this course are supplementary to the\nHonor Code. According to the St. Lawrence University Academic Honor Policy,\n\nIt is assumed that all work is done by the student unless the instructor/mentor/employer gives specific permission for collaboration.\nCheating on examinations and tests consists of knowingly giving or using or attempting to use unauthorized assistance during examinations or tests.\nDishonesty in work outside of examinations and tests consists of handing in or presenting as original work which is not original, where originality is required.\n\nClaims of ignorance and academic or personal pressure are unacceptable as excuses for academic dishonesty. Students must learn what constitutes one's own work and how the work of others must be acknowledged.\nFor more information, refer to www.stlawu.edu/acadaffairs/academic_honor_policy.pdf.\nTo avoid academic dishonesty, it is important that you follow all directions and collaboration rules and ask for clarification if you have any questions about what is acceptable for a particular assignment or exam. If I suspect academic dishonesty, a score of zero will be given for the entire assignment in which the academic dishonesty occurred for all individuals involved and Academic Honor Council will be notified. If a pattern of academic dishonesty is found to have occurred, a grade of 0.0 for the entire course can be given.\nPlease note that in addition the above, any assignments in which your score is reduced due to academic dishonesty will not be dropped according to the quiz policy e.g., if you receive a zero on a quiz because of academic dishonesty, it will not be dropped from your grade."
  },
  {
    "objectID": "index.html#tentative-schedule",
    "href": "index.html#tentative-schedule",
    "title": "DATA / STAT 334",
    "section": "Tentative Schedule",
    "text": "Tentative Schedule\n\n\n\nWeek\nDate\nTopics\n\n\n\n\n0\n1/17\nCore Concepts\n\n\n1\n1/22\nCore Concepts and Introduction to Git and GitHub\n\n\n2\n1/29\nDATA/STAT 234 Review and Assessment 1\n\n\n3\n2/5\nMapping and Expressing Uncertainty and Blog Post 1\n\n\n4\n2/12\nSTAT 213 Review and Assessment 2\n\n\n5\n2/19\nModel Visualization and Blog Post 2\n\n\n6\n2/26\nLogistic Regression and Assessment 3\n\n\n7\n3/4\nOther Topics, Ethics, and Blog Post 3\n\n\n8\n3/11\nInteractivity, Project Introduction, and Assessment 4\n\n\n\n\n\n\n\n\n3/18\nSpring Break\n\n\n\n\n\n\n\n9\n3/25\nShiny Introduction\n\n\n10\n4/2\nShiny Reactivity and Assessment 5\n\n\n11\n4/9\nProject\n\n\n12\n4/16\nProject\n\n\n13\n4/23\nProject\n\n\n14\n4/30\nProject\n\n\n\n\n\n\n\n\n\nThere will be no Final Exam, but keep your schedule open at our Final Exam time in case we decide to use it for something."
  },
  {
    "objectID": "00-choose-viz.html#why-visualize-data",
    "href": "00-choose-viz.html#why-visualize-data",
    "title": "1  Choosing a Visualization",
    "section": "\n1.1 Why Visualize Data?",
    "text": "1.1 Why Visualize Data?\nData visualization is important in many aspects of statistics and data science. Often, people can see patterns in data and understand the underlying data better with a visualization instead of numerical summary statistics.\nBut, isn’t it obvious how to create a “good” data visualization? Why should there be an entire course on the subject? Part of the answer is that there is some code that we need to learn to create flexible data visualizations. However, even if that were not the case, people are not necessarily good at constructing informative data visuals.\nExamine the data is ugly subreddit for a couple of minutes to see why this data visualization is not necessarily simply a natural talent for people.\nThroughout the rest of this section, we will use the Salaries data set from the carData package, which contains variables on the salaries of professors at a particular institution (not this institution though!), including the professor’s rank (AsstProf, AssocProf, or Prof), discipline (A for theoretical and B for applied), yrs.service, the years of service, sex (treated as binary in this data set: Male or Female), and salary (in dollars).\n\nlibrary(tidyverse)\nlibrary(carData)\nSalaries &lt;- Salaries |&gt; as_tibble() |&gt;\n  mutate(salary = salary / 1000)\ntheme_set(theme_minimal())\nSalaries\n#&gt; # A tibble: 397 × 6\n#&gt;   rank      discipline yrs.since.phd yrs.service sex   salary\n#&gt;   &lt;fct&gt;     &lt;fct&gt;              &lt;int&gt;       &lt;int&gt; &lt;fct&gt;  &lt;dbl&gt;\n#&gt; 1 Prof      B                     19          18 Male   140. \n#&gt; 2 Prof      B                     20          16 Male   173. \n#&gt; 3 AsstProf  B                      4           3 Male    79.8\n#&gt; 4 Prof      B                     45          39 Male   115  \n#&gt; 5 Prof      B                     40          41 Male   142. \n#&gt; 6 AssocProf B                      6           6 Male    97  \n#&gt; # ℹ 391 more rows\n\nWe will use an accompanying handout to discuss benefits and drawbacks of different graphs for various settings. The code for each graph in the handout is below, but, at this point in the course, we are focusing less on the code and more on what makes different graphics “good” or “bad.”"
  },
  {
    "objectID": "00-choose-viz.html#graphs-of-one-variable",
    "href": "00-choose-viz.html#graphs-of-one-variable",
    "title": "1  Choosing a Visualization",
    "section": "\n1.2 Graphs of One Variable",
    "text": "1.2 Graphs of One Variable\n\n1.2.1 Quantitative\nThe following are commonly used graphs for a single quantitative variable. In the handout, we will talk about whether we can clearly see the distribution center, distribution spread, distribution shape, and the sample size from each graph, as well as any relevant graphic parameters.\n\nggplot(data = Salaries, aes(x = salary)) +\n  geom_histogram(colour = \"black\", fill = \"white\") +\n  labs(title = \"Histogram\")\n\n\n\n\n\n\nggplot(data = Salaries, aes(x = salary)) +\n  geom_freqpoly() +\n  labs(title = \"Frequency Plot\")\n\n\n\n\n\n\nggplot(data = Salaries, aes(y = salary)) +\n  geom_boxplot() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank()) +\n  labs(title = \"Boxplot\")\n\n\n\n\n\n\nggplot(data = Salaries, aes(x = salary)) +\n  geom_density() +\n  labs(title = \"Density Plot\")\n\n\n\n\n\n1.2.2 Categorical\nThe following are commonly used graphs for a single categorical variable. In the accompanying handout, we will discuss whether we can clearly see the overall distribution and the sample size from each graph.\n\nggplot(data = Salaries, aes(x = rank)) +\n  geom_bar(colour = \"mediumpurple4\", fill = \"mediumpurple1\") +\n  labs(title = \"Bar Plot\")\n\n\n\n\n\n\nsalaries_sum &lt;- Salaries |&gt; group_by(rank) |&gt;\n  summarise(n = n()) |&gt;\n  mutate(prop = n / sum(n))\nggplot(data = salaries_sum, aes(x = rank, y = n)) +\n  geom_segment(aes(xend = rank, y = 0, yend = n),\n               colour = \"mediumpurple1\") +\n  geom_point(colour = \"mediumpurple4\") +\n  labs(title = \"Lollipop Plot\")\n\n\n\n\n\n\nggplot(data = salaries_sum, aes(x = rank, y = prop)) +\n  geom_col(colour = \"mediumpurple4\", fill = \"mediumpurple1\") +\n  labs(title = \"Bar Plot of Proportions\")\n\n\n\n\n\n\nggplot(data = salaries_sum, aes(x = \"\", y = n, fill = rank)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(\"y\") + scale_fill_viridis_d() +\n  theme_void() +\n  labs(title = \"Pie Chart\")"
  },
  {
    "objectID": "00-choose-viz.html#graphs-of-two-variables",
    "href": "00-choose-viz.html#graphs-of-two-variables",
    "title": "1  Choosing a Visualization",
    "section": "\n1.3 Graphs of Two Variables",
    "text": "1.3 Graphs of Two Variables\n\n1.3.1 Quantitative and Categorical\nThe following are commonly used graphs for a quantitative variable and categorical variable. In the handout, we will talk about whether we can clearly compare the distribution centers, distribution spreads, distribution shapes, and the sample sizes in each level of the categorical variable.\n\nggplot(data = Salaries, aes(x = discipline, y = salary)) +\n  geom_boxplot() +\n  labs(title = \"Side-by-Side Boxplots\")\n\n\n\n\n\n\nggplot(data = Salaries, aes(x = discipline, y = salary)) +\n  geom_violin() +\n  labs(title = \"Side-by-Side Violin Plots\")\n\n\n\n\n\n\nggplot(data = Salaries, aes(x = salary)) +\n  geom_freqpoly(aes(colour = discipline)) +\n  scale_colour_viridis_d() +\n  labs(title = \"Coloured Frequency Plots\")\n\n\n\n\n\n\nggplot(data = Salaries, aes(x = salary)) +\n  geom_histogram(colour = \"black\", fill = \"white\") +\n  facet_wrap(~discipline, ncol = 1) +\n  labs(title = \"Faceted Histograms\")\n\n\n\n\n\n\nggplot(data = Salaries, aes(x = salary)) +\n  geom_density(aes(fill = discipline, colour = discipline), alpha = 0.50) +\n  scale_fill_viridis_d() + scale_colour_viridis_d() +\n  labs(title = \"Coloured Density Plots\")\n\n\n\n\n\n\nggplot(data = Salaries, aes(x = discipline, y = salary)) +\n  geom_jitter(width = 0.15) +\n  labs(title = \"Strip Plot\")\n\n\n\n\n\n\nsalary_med &lt;- Salaries |&gt; group_by(discipline) |&gt;\n  summarise(med_sal = median(salary))\n## as discussed in handout, this is a horrible way to visualize this data!\nggplot(salary_med, aes(x = discipline, y = med_sal)) +\n  geom_col(colour = \"mediumpurple4\", fill = \"mediumpurple1\") +\n  labs(title = \"Bar Plot of Median\", y = \"Median Salary\")\n\n\n\n\n\n\nlibrary(ggbeeswarm)\nggplot(data = Salaries, aes(x = discipline, y = salary)) +\n  geom_beeswarm() +\n  labs(title = \"Beeswarm Plot\")\n\n\n\n\n\n1.3.2 Two Categorical\nThe following are commonly used graphs for two categorical variables. In the handout, we will talk about whether we can clearly assess whether there is any evidence of an association between the two variables as well as whether we can see the sample size for each combination of the levels of the two categorical variables.\n\nggplot(data = Salaries, aes(x = discipline, fill = sex)) +\n  geom_bar() +\n  scale_fill_viridis_d() +\n  labs(title = \"Stacked Bar Plot\")\n\n\n\n\n\n\nggplot(data = Salaries, aes(x = discipline, fill = sex)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_viridis_d() +\n  labs(title = \"Stacked Proportional Bar Plot\")\n\n\n\n\n\n\nggplot(data = Salaries, aes(x = discipline, fill = sex)) +\n  geom_bar(position = \"dodge\") +\n  scale_fill_viridis_d() +\n  labs(title = \"Side-by-Side Bar Plot\")\n\n\n\n\n\n\nsalaries_heat &lt;- Salaries |&gt; group_by(discipline, sex) |&gt;\n  summarise(n = n())\nggplot(data = salaries_heat, aes(x = discipline, y = sex, fill = n)) +\n  geom_tile() + scale_fill_viridis_c() +\n  labs(title = \"Heat Map\")\n\n\n\n\n\n1.3.3 Two Quantitative\nThe following are commonly used graphs for two quantitative variables. In the handout, we will talk about whether we can clearly assess trend and whether we can see the sample size in each graph.\n\nggplot(data = Salaries, aes(x = yrs.service, y = salary)) +\n  geom_point() +\n  labs(title = \"Scatterplot\")\n\n\n\n\n\n\nggplot(data = Salaries, aes(x = yrs.service, y = salary)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Scatterplot with Linear Regression\")\n\n\n\n\n\n\nggplot(data = Salaries, aes(x = yrs.service, y = salary)) +\n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Scatterplot with Smoother\")"
  },
  {
    "objectID": "00-choose-viz.html#graphs-of-more-than-two-variables",
    "href": "00-choose-viz.html#graphs-of-more-than-two-variables",
    "title": "1  Choosing a Visualization",
    "section": "\n1.4 Graphs of More Than Two Variables",
    "text": "1.4 Graphs of More Than Two Variables\nFor more than two variables, we need to map the third variable to something on the plot other than the x-axis and the y-axis. The two most common ways to incorporate a third variable into a plot are:\n\nuse colour to colour by the third variable\nfacet by the third variable\n\nBelow are a couple of examples. Next week, we will discuss benefits and drawbacks of using colour vs. faceting.\n\n1.4.1 Two Quantitative and One Categorical\n\nggplot(data = Salaries, aes(x = yrs.service, y = salary)) +\n  geom_point(aes(colour = sex)) +\n  geom_smooth(aes(colour = sex)) +\n  scale_colour_viridis_d() +\n  labs(title = \"Colour by Third Variable\")\n\n\n\n\n\n\nggplot(data = Salaries, aes(x = yrs.service, y = salary)) +\n  geom_point() +\n  facet_wrap(~ sex) +\n  geom_smooth() +\n  labs(title = \"Facet by Third Variable\")\n\n\n\n\n\n1.4.2 Two Categorical and One Quantitative\n\nggplot(data = Salaries, aes(x = salary)) +\n  geom_histogram(colour = \"black\", fill = \"white\") +\n  facet_grid(discipline ~ rank) +\n  labs(title = \"Facet by Two Variables\")\n\n\n\n\n\n\nggplot(data = Salaries, aes(x = salary)) +\n  geom_freqpoly(aes(colour = discipline), bins = 15) +\n  facet_wrap(~ rank) +\n  scale_colour_viridis_d() +\n  labs(title = \"Facet by One Variable, Colour by One Variable\")"
  },
  {
    "objectID": "01-concepts.html#data-visualization-concepts-class-prep",
    "href": "01-concepts.html#data-visualization-concepts-class-prep",
    "title": "2  Core Viz Concepts",
    "section": "\n2.1 Data Visualization Concepts (Class Prep)",
    "text": "2.1 Data Visualization Concepts (Class Prep)\nRead Sections 1.1 - 1.2 of Kearen Healy’s Data Visualization: A Practical Introduction, found here. As you read, answer the following questions in just 1 to 2 sentences in a Quarto document. Recall that you should change the YAML header of your Quarto document so that all resources are embedded:\n---\ntitle: \"Your Title\"\nauthor: \"Your Name\"\nformat: \n  html:\n    embed-resources: true\n---\n\nUsing either Anscombe’s quartet or the income/voter turnout graph as an example, explain why it’s valuable to look at data graphically instead of examining data only with summary statistics.\n\n\n\nTake a look at the bar plot in Figure 1.4. Give a couple of reasons for why the chart has “bad taste.”\n\n\n\nWhy might you not always want to maximize the data-to-ink ratio when making charts?\n\n\n\nWhat do the authors mean when they say that “relative comparisons need a stable baseline” and how does that affect your ability to interpret the coloured stacked bar plot in Figure 1.11?\n\n\n\nWhat are two key takeaways from Sections 1.1 and 1.2?\n\n\n\nWhat is one question that you have about the reading?"
  },
  {
    "objectID": "01-concepts.html#grammar-of-graphics",
    "href": "01-concepts.html#grammar-of-graphics",
    "title": "2  Core Viz Concepts",
    "section": "\n2.2 Grammar of Graphics",
    "text": "2.2 Grammar of Graphics\nWe begin with a review of the ggplot2 package. However, we will now consider the package from a more conceptual perspective through the idea of a Grammar of Graphics. According to Hadley Wickham in A Layered Grammar of Graphics , a Grammar of Graphics is\n\n“a tool that enables us to concisely describe the components of a graphic. Such a grammar allows us to move beyond named graphics (e.g., the”scatterplot”) and gain insight into the deep structure that underlies statistical graphics.”\n\nIn STAT/DATA 234, we used the ggplot2 package, which implements a grammar of graphics in R. There are 7 parameters in a grammar of graphics:\n\n\n&lt;DATA&gt;: the data set\n\n&lt;MAPPINGS&gt;: an aesthetic mapping\n\n&lt;GEOM_FUNCTION&gt;: a geometrical object\n\n&lt;STAT&gt;: a statistical transformation\n\n&lt;POSITION&gt;: a position adjustment\n\n&lt;FACET_FUNCTION&gt;: subplots\n\n&lt;COORDINATE_FUNCTION&gt;: a coordinate system\n\nSo, what makes these 7 parameters particularly useful. The R for Data Science book (https://r4ds.had.co.nz/) states that\n\n“you can uniquely describe any plot as a combination of a dataset, a geom, a set of mappings, a stat, a position adjustment, a coordinate system, and a faceting scheme.”\n\nFortunately, ggplot2 (and most implementations of the grammar of graphics) provide useful defaults for some of the 7 parameters, so often you don’t need to specify all 7. Recall that, in STAT/DATA 234, the only 3 that were (almost) always supplied were &lt;DATA&gt;, &lt;MAPPINGS&gt;, and &lt;GEOM_FUNCTION&gt;.\n\n2.2.1 &lt;DATA&gt;, &lt;MAPPINGS&gt;, and &lt;GEOM_FUNCTION&gt;\n\nThese three parameters were the most heavily used in the Data Science class. Therefore, the explanation here will be very brief. If you need additional review, examine https://highamm.github.io/datascience234/ggplot2.html.\n&lt;DATA&gt; is a required parameter and is simply the data set that you will use for the plot.\n&lt;MAPPINGS&gt; are what is specified in the aes() aesthetics function. They map variables in your data set to plot characteristics. Common mappings are\n\n\nx position\n\ny position\n\ncolour,\n\nsize,\n\nshape,\n\ngroup, and\n\nfill.\n\n&lt;GEOM_FUNCTION&gt; is the geometric object used to represent the data. Common examples include\n\n\ngeom_histogram() and geom_freqpoly() for a single quantitative variable\n\ngeom_bar() and geom_col() for a single categorical variable\n\ngeom_point(), geom_line(), and geom_smooth() for two quantitative variables\n\ngeom_boxplot() and geom_violin() for a categorical and a quantitative variable\n\ngeom_bar(), geom_tile(), and geom_bin_2d() for two categorical variables.\n\nOther geoms, like geom_text() and geom_label() allow you to annotate a plot. And there are many others: geom_hexbin(), geom_area(), and geom_hline() are a few of the more useful ones.\nLet’s practice making some basic plots with these 3 parameters using the penguins data set in the palmerpenguins library. The data set contains observations on 344 penguins. Penguin species, island, bill_length_mm, sex, and a few other measurements are recorded.\n\n## install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(tidyverse)\ntheme_set(theme_minimal())\npenguins\n#&gt; # A tibble: 344 × 8\n#&gt;   species island   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#&gt;   &lt;fct&gt;   &lt;fct&gt;             &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n#&gt; 1 Adelie  Torgers…           39.1          18.7               181        3750\n#&gt; 2 Adelie  Torgers…           39.5          17.4               186        3800\n#&gt; 3 Adelie  Torgers…           40.3          18                 195        3250\n#&gt; 4 Adelie  Torgers…           NA            NA                  NA          NA\n#&gt; 5 Adelie  Torgers…           36.7          19.3               193        3450\n#&gt; 6 Adelie  Torgers…           39.3          20.6               190        3650\n#&gt; # ℹ 338 more rows\n#&gt; # ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\nWe can combine the &lt;DATA&gt;, &lt;MAPPINGS&gt;, AND &lt;GEOM&gt; components of the grammar of graphics to make many of the plots you constructed in STAT/DATA 234. In the following plots, the other 4 parameters of the grammar of graphics (which we will discuss next) are set by default.\n\n## single quantitative frequency plot\nggplot(data = penguins, aes(x = bill_length_mm)) +\n  geom_freqpoly() \n\n\n\n\n## single categorical variable\nggplot(data = penguins, aes(x = species)) +\n  geom_bar(colour = \"mediumpurple4\", fill = \"mediumpurple1\")\n\n\n\n\n## two quantitative variables\nggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n## two categorical variables\nggplot(data = penguins, aes(x = species, y = island)) +\n  geom_bin_2d()\n\n\n\n\n\n2.2.2 &lt;FACET&gt;\n\nThe other parameter that we commonly used in Data Science is the &lt;FACET&gt; parameter, often through the facet_wrap() function or the facet_grid() function. As a reminder, faceting is useful if you would like to make subplots for different subsets of the data.\nFor example, we might make faceted histograms of bill_length_mm by species instead of a coloured frequency plot:\n\nggplot(data = penguins, aes(x = bill_length_mm)) +\n  geom_histogram(colour = \"black\", fill = \"white\", bins = 15) +\n  facet_wrap(~ species)\n\n\n\n\nOr, we might make faceted scatterplots by species instead of colouring by species.\n\nggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(~ species)\n\n\n\n\nThe previous graph now has 4 Grammar of Graphics parameters explicitly specified: &lt;DATA&gt;, &lt;MAPPINGS&gt;, &lt;GEOM&gt;, and &lt;FACET&gt;.\n\n2.2.3 &lt;STAT&gt;, &lt;POSITION&gt;, &lt;COORDINATE_FUNCTION&gt;\n\nThe remaining 3 parameters in the Grammar of Graphics are &lt;STAT&gt;, &lt;POSITION&gt;, &lt;COORDINATE_FUNCTION&gt;. Often, these do not need to be specified at all because the ggplot2 package provides useful defaults for them. In most situations, these defaults do not need to be changed, but we will see one illustrative example of each to help better understand what they are.\n\n&lt;STAT&gt;\nEach geom has a default statistical transformation, &lt;STAT&gt;, given in the help file for that &lt;GEOM&gt;. In many cases, the transformation is the “identity” (meaning there is no real transformation at all). For geom_bar(), the default &lt;STAT&gt; is count: a bar plot is made by counting up the number of each level of a categorical variable. Behind the scenes, using geom_bar() creates a new data frame that counts the number of observations for each unique level of x and plots that.\n\nggplot(data = penguins, aes(x = species)) +\n  geom_bar(stat = \"count\") ## don't need stat = \"count\" because it's the default.\n\n\n\n\nNow consider a data frame in which species are already counted. We will make this data frame as a class and then use it in the following code to see how we can make a barplot from this data set with geom_bar().\n\nggplot(data = INSERT_DATA, aes(x = species, y = INSERT_YVAR)) +\n  geom_bar(stat = \"change_default_stat\") \n\n\n&lt;POSITION&gt;:\nA common way in which &lt;POSITION&gt; is changed is to create a side-by-side barplot instead of a stacked barplot. The default position in geom_bar() is \"stack\" for an observations that occupy the same x:\n\nggplot(data = penguins, aes(x = species, fill = sex)) +\n  geom_bar(position = \"stack\") + ## don't need \"stack\" because it's default\n  scale_fill_viridis_d()\n\n\n\n\nAnother option for &lt;POSITION&gt; is \"dodge\":\n\nggplot(data = penguins, aes(x = species, fill = sex)) +\n  geom_bar(position = \"dodge\") + \n  scale_fill_viridis_d()\n\n\n\n\nWhy are the bars in position = \"dodge\" a different width? We will investigate this as a class.\nAnother example of a &lt;POSITION&gt; adjustment is using \"jitter\" instead of \"identity\" in geom_point() to add a little noise to overlapping data points.\n\n&lt;COORDINATE_FUNCTION&gt;:\nFinally, most plots use a standard Cartesian &lt;COORDINATE_FUNCTION&gt;. This rarely needs to be changed, but we will see coord_quickmap() used to fix the aspect ratio of spatial maps. Another &lt;COORDINATE_FUNCTION&gt; is coord_polar():\n\nggplot(data = penguins, aes(x = species)) +\n  geom_bar() +\n  coord_polar()\n\n\n\n\nI can’t think of a single time I have ever used coord_polar() but a more useful coordinate function is coord_flip(), which you likely used in STAT/DATA 234 to flip the x and y coordinates of a Cartesian coordinate system:\n\nggplot(data = penguins, aes(x = species)) + \n  geom_bar() +\n  coord_flip() \n\n\n\n\nExercise 1. The following scatterplot explicitly specifies 3 of the 7 grammar of graphics parameters.\n\nggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point() \n\n\n\n\nThe other 4 parameters are given default values: &lt;FACET&gt; is facet_null() (which is added as another layer to the plot), &lt;STAT&gt; is \"identity\" (an argument of geom_point()), &lt;POSITION&gt; is \"identity\" (an argument of geom_point()), and &lt;COORDINATE_FUNCTION&gt; is coord_cartesian() (which is added as another layer to the plot). Modify the plot so that it explicitly uses the default values for the other 4 parameters."
  },
  {
    "objectID": "01-concepts.html#your-turn",
    "href": "01-concepts.html#your-turn",
    "title": "2  Core Viz Concepts",
    "section": "\n2.3 Your Turn",
    "text": "2.3 Your Turn\nExercise 1. Make another plot of bill_length_mm with a different geom that is used for a single quantitative variable.\nExercise 2. Create a plot of a quantitative and a categorical variable in the penguins data set.\nExercise 3. Modify the frequency plot made with geom_freqpoly() to use colour in two different ways:\n\nchange the colour of the line to any colour that R knows. A list of some colours can be found at this link.\nadd a colour aesthetic to make three different frequency lines, one for each species of penguin.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nRecall that only aesthetic mappings (variables) go inside aes(). Of (a) or (b), which one is an aesthetic mapping?\n\n\n\nExercise 4. Recall that aes() aesthetics specified within the ggplot() function directly are called global aesthetics because every other &lt;GEOM&gt; will use them (unless specifically overridden) while aes() specified within a particular &lt;GEOM&gt; are called local aesthetics because only that particular &lt;GEOM&gt; will use them.\nModify the scatterplot and smoother so that\n\nthe points are coloured by species, but there is only one smoother instead of three.\nthere are three different coloured smoothers (one for each species) but the points are all the same colour.\n\nExercise 5. Construct another graph that explicitly uses all 7 grammar of graphics parameters (you may set some of these parameters to be the default values, but you should do so explicitly).\nExercise 6. Create two different graphs that differ only by 1 of the 7 grammar of graphics parameters. Then, show your two graphs (but not your code) to a partner and see if they can identify which graphics parameter has been modified."
  },
  {
    "objectID": "04-concepts-applied.html#more-data-visualization-concepts-class-prep",
    "href": "04-concepts-applied.html#more-data-visualization-concepts-class-prep",
    "title": "3  Applied Concepts",
    "section": "\n3.1 More Data Visualization Concepts (Class Prep)",
    "text": "3.1 More Data Visualization Concepts (Class Prep)\nRead Sections 1.3 - 1.7 of Kearen Healy’s Data Visualization: A Practical Introduction, found here. As you read, answer the following questions in just 1 to 2 sentences.\n\nWhat is the difference between a colour’s hue and a colour’s intensity?\n\n\n\nThink of an example where you would want to use a sequential colour scale that’s different from the one given in the text. Then, think of examples where you would use a diverging colour scale and an unordered colour scale.\n\n\n\nSome gestalt inferences take priority over others. Using Figure 1.21, give an example of a gestalt inference that takes priority over another one.\n\n\n\n“Bar charts are better than Pie charts for visualizing a single categorical variable.” Explain how results shown in Figure 1.23 support this claim.\n\n\n\nSuppose you have a scatterplot of height on the y-axis vs. weight on the x-axis for people born in Canada, Mexico, and the United States. You now want to explore whether the relationship is different for people born in the United States, people born in Canada, and people born in Mexico. Should you use different shapes to distinguish the three countries or should you use different colours? Explain using either Figure 1.24 or 1.25.\n\n\n\nWhen might you use the left-side of Figure 1.27 to show the law school data? When might you use the right-side of Figure 1.27 to show the law school data?\n\n\n\nSummary: What are two takeaways from Sections 1.3-1.7?\nWhat is one question that you have about the reading?"
  },
  {
    "objectID": "04-concepts-applied.html#examples",
    "href": "04-concepts-applied.html#examples",
    "title": "3  Applied Concepts",
    "section": "\n3.2 Examples",
    "text": "3.2 Examples\nWe will examine pairs of visualizations. For each pair, we will (1) decide if there is a superior visualization for that context and (2) relate our decision, as much as possible, to the concepts from the Healy reading. These situations are certainly not meant to cover all scenarios you will run into while visualizing data. Instead, they cover a few common situations.\nWe will use the penguins data set from the palmerpenguins package. Each row corresponds to a particular penguin. Categorical variables measured on each penguin include species, island, sex, and year. Quantitative variables include bill_length_mm, bill_depth_mm, and body_mass_g.\nExample 1: Examine the following plots, which attempt to explore how many different penguins were measured in each year of the study for each species.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\ntheme_set(theme_minimal())\n\npenguins_sum &lt;- penguins |&gt; group_by(species, year) |&gt;\n  summarise(n_penguins = n()) |&gt;\n  mutate(year = factor(year))\n\nggplot(data = penguins_sum, aes(x = year, fill = species)) +\n  geom_col(aes(y = n_penguins)) +\n  theme_minimal() +\n  scale_fill_viridis_d()\n\n\n\n\nggplot(data = penguins_sum, aes(x = year, y = n_penguins,\n                                colour = species, group = species)) +\n  geom_line() +\n  theme_minimal() +\n  scale_colour_viridis_d()\n\n\n\n\nWhich plot is preferable? Can you relate your choice to a concept in the reading?\n\n\nExample 2: The following plots examine the number of penguins in each species in the data set.\n\nggplot(data = penguins, aes(x = species)) +\n  geom_bar(fill = \"darkslategray4\") +\n  theme_minimal()\n\n\n\n\nggplot(data = penguins, aes(x = species)) +\n  geom_bar(fill = \"darkslategray4\") +\n  coord_cartesian(ylim = c(50, 160)) +\n  theme_minimal()\n\n\n\n\nWhich plot is preferable? Can you relate your choice to a concept in the reading?\n\n\nExample 3: The following plots examine the distribution of body_mass_g for each species of penguin.\n\nlibrary(ggbeeswarm)\nggplot(data = penguins, aes(x = species, y = body_mass_g)) +\n  geom_beeswarm(alpha = 0.7) +\n  theme_minimal()\n\n\n\n\nggplot(data = penguins, aes(x = species, y = body_mass_g)) +\n  geom_beeswarm(alpha = 0.7) +\n  theme_minimal() +\n  ylim(c(0, 6500))\n\n\n\n\nWhich plot is preferable? Can you relate your choice to a concept in the reading?\n\n\nExample 4: The following plots explore whether to use colour or faceting to explore three (or more) variables in a data set.\n\n\n\n\n\n\nTip\n\n\n\nWhether you want to use colour or use faceting often depends on two things:\n\nhow many categories are there? The more categories, the better faceting is, in general.\nhow clumped are points within one category? If each category has nicely clumped points, colour works better, but, if there is a lot of overlap across categories, faceting generally works better.\n\n\n\nPair 1. These plots are meant to explore the relationship between bill depth and bill length for each species of penguin.\n\nggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(colour = species)) +\n  scale_colour_viridis_d() +\n  theme_minimal()\n\n\n\n\nggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point() +\n  facet_wrap( ~ species) +\n  theme_minimal() \n\n\n\n\nWhich plot is preferable? Why?\n\n\nPair 2: These plots are meant to explore the relationship between bill length and bill depth for each species-island-sex combination.\n\npenguins &lt;- penguins |&gt; mutate(species_island_sex = interaction(species, \n                                                                island,\n                                                                sex))\nggplot(data = penguins |&gt;\n         filter(!is.na(sex)), aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(colour = species_island_sex)) +\n  scale_colour_viridis_d() +\n  theme_minimal()\n\n\n\n\nggplot(data = penguins |&gt; filter(!is.na(sex)), \n                                 aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point() +\n  facet_wrap(~ species_island_sex) +\n  theme_minimal()\n\n\n\n\nBoth plots can be improved upon but which plot is preferable? Why?\n\nOn a sheet of paper, sketch out an improved plot to investigate these variables.\n\n\nExample 5: The following plots look at the distribution of island.\n\nggplot(data = penguins, aes(x = island)) +\n  geom_bar(fill = \"darkslategray4\") +\n  theme_minimal()\n\n\n\n\npenguins_island &lt;- penguins |&gt; count(island)\nggplot(penguins_island, aes(x = \"\", y = n, fill = island)) +\n  geom_bar(stat = \"identity\", width = 1, colour = \"white\") +\n  coord_polar(\"y\", start = 0) +\n  \n  theme_void() +\n  scale_fill_viridis_d()\n\n\n\n\nWhich plot is preferable? Can you relate your choice to a concept in the reading?\n\n\nExample 6: The following plot uses the Happy Planet Index data to show the top 10 and bottom 10 countries for the HappyPlanetIndex variable, a variable that is meant to measure how “happy” its citizens while taking into account how the average citizen from that country affects our planet.\n\nlibrary(tidyverse)\nlibrary(here)\nhpi_df &lt;- read_csv(here(\"data/hpi-tidy.csv\"))\nhpi_extreme &lt;- hpi_df |&gt;\n  arrange(desc(HappyPlanetIndex)) |&gt;\n  slice(1:10, (nrow(hpi_df) - 9):nrow(hpi_df)) |&gt;\n  mutate(Country = fct_reorder(Country, HappyPlanetIndex))\n\nggplot(data = hpi_extreme, aes(x = Country, y = HappyPlanetIndex,\n                               fill = Region)) +\n  geom_col() +\n  scale_fill_viridis_d() +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\nggplot(data = hpi_extreme, aes(x = Country, y = HappyPlanetIndex,\n                               colour = Region)) +\n  geom_point() +\n  geom_segment(aes(xend = Country, y = 0, yend = HappyPlanetIndex)) +\n  scale_colour_viridis_d() +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\nWhich plot is preferable? Can you relate your choice to a concept in the reading?\n\n\nExample 7: The final few examples discuss the choice of colour palettes when using colour in visualizations. We want to use our graphics to communicate with others as clearly as possible. We also want to be as inclusive as possible in our communications. This means that, if we choose to use colour, our graphics should be made so that a colour-vision-deficient (CVD) person can read our graphs.\n\n\n\n\n\n\nNote\n\n\n\nAbout 4.5% of people are colour-vision-deficient, so it’s actually quite likely that a CVD person will view the graphics that you make (depending on how many people you share it with) reference here.\n\n\nThe colour scales from R Colour Brewer are readable for common types of CVD. A list of scales can be found here.\nYou’ve already read about different types of colour ordering in our course reading. We see some scales for each of the three categories (sequential, diverging, and unordered) in R Colour Brewer. You would typically use the top scales if the variable you are colouring by is ordered sequentially (called seq for sequential), the bottom scales if the variable is diverging (called div for diverging, like Republican / Democrat lean so that the middle is colourless), and the middle set of scales if the variable is not unordered and is categorical (called qual for qualitative like the names of different treatment drugs for a medical experiment).\n\nggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(colour = species)) +\n  theme_minimal() +\n  scale_colour_brewer(type = \"seq\")\n\n\n\n\nggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(colour = species)) +\n  theme_minimal() +\n  scale_colour_brewer(type = \"div\")\n\n\n\n\nggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(colour = species)) +\n  theme_minimal() +\n  scale_colour_brewer(type = \"qual\")\n\n\n\n\nFor this example, which scale makes the most sense: seq, div, or qual? Why?\n\nAnother colour and fill scale package is the viridis package. The base viridis functions automatically load with ggplot2 so there’s no need to call the package with library(viridis). The viridis colour scales were made to be both aesthetically pleasing, CVD-friendly, and visible when printed to black-and-white. We’ve used the viridis colour scale in many examples already but another example is given below, in which the default option argument is changed to another scale called \"plasma\". The viridis colour scales can either be used as sequential colour scales or qualitative colour scales, but I do not believe they have any colour scales that would work well for diverging colours.\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_viridis_d(option = \"plasma\") +\n  theme_minimal()"
  },
  {
    "objectID": "04-concepts-applied.html#your-turn",
    "href": "04-concepts-applied.html#your-turn",
    "title": "3  Applied Concepts",
    "section": "\n3.3 Your Turn",
    "text": "3.3 Your Turn\nExercise 1. Consider a data set on Covid 19 cases in counties across the United States. For each county, we have the following variables:\n\nnumber of cases\nchange in the number of cases from the previous week of the year (negative if cases have decreased, positive if cases have increased)\nwhether the county voted Republican or Democrat in the most recent presidential election\npopulation population change in the last decade (a 1.2% change would indicate that the population in the county increased by 1.2%)\n\nNow suppose that you want to construct map of the counties, filling the area of the county with one of the variables.\n\nFor the number of cases variable, would you use a sequential, diverging, or qualitative fill colour scale?\nFor the change in the number of cases variable, would you use a sequential, diverging, or qualitative fill colour scale?\nFor the political party variable, would you use a sequential, diverging, or qualitative fill colour scale?\nFor the percent population change variable, would you use a sequential, diverging, or qualitative fill colour scale?\n\n\nExercise 2. Read the examples section of the Help file for ?scale_colour_viridis_d and examine the difference between scale_colour_viridis_d(), ?scale_colour_viridis_c(), and scale_colour_viridis_b(). Explain what the difference is between the three functions.\nExercise 3 We can also specialize the plot’s theme. There are a ton of options with the theme() function to really specialise your plot.\nConsider the coloured scatterplot of the Happy Planet Index data:\n\nggplot(data = hpi_df, aes(x = Footprint, y = Wellbeing, colour = Region)) +\n  geom_point()\n\n\n\n\nUsing only options in theme() or options to change colours (scale_colour_manual()), shapes, sizes, etc., create the ugliest possible ggplot2 graph that you can make. You may not change the underlying data for this graph, and the final version of the graph must still be interpretable (e.g. you can’t get rid of or obstruct the points so much that you can no longer see the pattern). The goal here is to investigate some of the options given in theme() and the other scales, and to have a brief refresher on the structure and syntax of ggplot().\nYou can examine all of the theme options here: https://ggplot2.tidyverse.org/reference/theme.html. Note that there are examples at the very bottom of the page that might be helpful to take a look at."
  },
  {
    "objectID": "02-software.html#installing-r-and-r-studio-class-prep",
    "href": "02-software.html#installing-r-and-r-studio-class-prep",
    "title": "4  Software",
    "section": "\n4.1 Installing R and R Studio (Class Prep)",
    "text": "4.1 Installing R and R Studio (Class Prep)\nIf you had Professor Higham for STAT/DATA 234, you should already have R and R Studio installed on your laptop computer. You may also already have these installed from another course or an independent study. Even though you might already have these installed, it might be time for an update: if you have not reinstalled these in a year, updating would be a good idea.\nThe following videos provide instructions on how to install R and R Studio to your laptop computer. It will be easiest if you complete all of these steps consecutively in one sitting. So, you should ideally have about 45 minutes to an hour of your time blocked off to complete the “getting started” process using the videos below.\n Watch and follow along with a video on installing R. \n Watch and follow along with a video on installing R Studio. \n Watch and follow along with a video on installing R packages and changing other options. \nAfter you install R Studio, create a new Quarto file (File -&gt; New File -&gt; Quarto Document), and answer the following questions within the file. At the top of the file, you should change the YAML header to something like\n---\ntitle: \"Your Title\"\nauthor: \"Your Name\"\nformat: \n  html:\n    embed-resources: true\n---\nThe embed-resources part of the YAML header says to embed all of the necessary code, tables, and graphs into one single file.\nRender the file and submit the resulting .html file to Canvas.\n\nWhat is your name and what is your class year (first-year, sophomore, junior, senior)?\nWhat is/are your major(s) and minor(s), either actual or intended?\nWhy did you decide to take this course?\nIn what semester and year did you take STAT/DATA 234? STAT 213?\nHave you taken CS 140?\nHave you used Git before? If so, in what context?\nBefore this course, did you use R Studio on your desktop?\nWhat is your hometown: city, state, country?\nDo you play a sport on campus? If so, what sport? If not, what is an activity that you do on or off-campus?\nWhat is your favorite TV show or movie or band/musical artist?\nTell me something about yourself.\nWhat are your expectations for this class and/or what do you hope to gain from this class?"
  },
  {
    "objectID": "02-software.html#installing-git-and-using-github-class-prep",
    "href": "02-software.html#installing-git-and-using-github-class-prep",
    "title": "4  Software",
    "section": "\n4.2 Installing Git and Using GitHub (Class Prep)",
    "text": "4.2 Installing Git and Using GitHub (Class Prep)\n\n\n\n\n\n\nImportant\n\n\n\nStart this early, and, if you get stuck, try to troubleshoot for 5 minutes or so. If you are still stuck, to protect your own time, make an appointment for office hours so we can work through the issue.\n\n\nThe following links provide instructions on getting started with Git and GitHub for version control of projects (repositories). The videos assume that you already have R and R Studio installed on your laptop. It will be easiest if you complete all of these steps consecutively in one sitting. So, you should ideally have about 1.5 - 2 hours of your time blocked off to complete the “getting started” process using the videos below. It might also be helpful (but not necessary) to work through these with a partner or two in case either of you get stuck.\nThis section outlines the steps to get Git up, running, and connected to R Studio. There are accompanying videos at https://highamm.github.io/dataviz334/02-software.html#installing-git-and-using-github-class-prep, which may provide a little more guidance than the written steps below.\n\n4.2.1 Pre-requisites\n\nInstall both R and R Studio locally.\n\nInstall Git, following https://happygitwithr.com/install-git.html. Note that most of the code in this section is code for the Terminal. You can access the Terminal within R Studio by going to “Tools -&gt; Terminal -&gt; New Terminal”\n\nYou may already have Git installed: type git --version in your Terminal window and hit Enter: if you see a version number get returned, you can jump to the next step!\nMake sure to follow the directions with your associated operating system (likely either Windows or Mac).\nAfter installation, in the Terminal, type git --version to verify that you get a version number\n\n\nCreate a GitHub account at https://github.com/ with the “Sign Up” button.\n\n4.2.2 Connect GitHub to R Studio\n\nWhile the previous steps were completed in Terminal, the following steps use R code itself, so you can run the commands below like normal R code.\n\nInstall the usethis package in R with install.packages(\"usethis\").\n\nGive Git your credentials with the use_git_config function in the usethis package: usethis::use_git_config(user.name = \"Matt Higham\", user.email = \"mhigham@stlawu.edu\").\n\nNote that the email provided must match your GitHub email address. The user.name need not match.\n\n\n\nGenerate a Personal Access Token (PAT) with usethis::create_github_token()\n\nYou can change the recommended scopes if you want but you should at least leave “repo”, “user”, and “workflow” checked.\nBy default, the PAT will expire in 30 days. People have varying levels of comfort with security: I will somewhat shamefully admit that I always click the “No Expiration” option because I do not want to ever generate a new one.\n\n\n\nInstall the gitcreds package with install.packages(\"gitcreds\") and then, in R, run gitcreds::gitcreds_set() and paste in your PAT when prompted.\n\nRun gitcreds::gitcreds_get() or usethis::git_sitrep() to verify that your PAT was accepted.\n\n\n\n4.2.3 Create a Repository\n\n\nOn your GitHub account site, click the “+” sign in the upper-right corner and select “New Repository.”\n\nGive the repository a title and make sure to have the option to create the README file checked.\nIf your repository will eventually contain data or other code that cannot be made public, you should select the “Private” option for the repository.\n\n\n\nBack in R Studio, go to “File -&gt; New Project”, select “Version Control” from the options, and then select “Git”. Then, copy and paste the GitHub repository web URL address, give the folder that will be created a name (giving it the same name as the repository is not required but is something that I find convenient), select where that folder will live on your local computer, and click “Create Project”.\n\nIf all goes well, you should have a new project with a “Git” pane in the upper-right window. If all does not go well, it may be that R Studio cannot locate Git on your local computer and that we need to do a bit of troubleshooting.\n\n\n\n4.2.4 Commit, Push, and Pull\n\nWatch and follow along with a  video on making commits, pushing, and pulling.\n\nAfter you are able to commit, push, and see that push show up on your remote GitHub site, you know that you have successfully set up this version control system!"
  },
  {
    "objectID": "02-software.html#using-git-and-github",
    "href": "02-software.html#using-git-and-github",
    "title": "4  Software",
    "section": "\n4.3 Using Git and GitHub",
    "text": "4.3 Using Git and GitHub\nIn class, we will discuss how to use Git and GitHub with R Studio as well as how we will use these tools to submit assignments in this course. By the end of the class in which we discuss Git and GitHub, at minimum you should be able to (1) locally commit a change to your files with a commit message, (2) push that local change to your remote GitHub repository, and (3) pull a change that someone else made to the remote GitHub repository."
  },
  {
    "objectID": "03-statdata234.html#review-with-hpi-data-class-prep",
    "href": "03-statdata234.html#review-with-hpi-data-class-prep",
    "title": "5  DATA/STAT 234 Review",
    "section": "\n5.1 Review with HPI Data (Class Prep)",
    "text": "5.1 Review with HPI Data (Class Prep)\nThe data set that we will use is the Happy Planet Index: you may have encountered this data set before, but we are using it again here because (1) it’s really cool and (2) it has a lot of nice properties for basic data visualization (variables to colour by and facet by, ways to label points that make sense, etc.).\nThe Happy Planet Index (HPI) is a measure of how efficiently a country uses its ecological resources to give its citizens long “happy” lives. You can read more about this data here.\nThe basic idea is that the HPI is a metric that computes how happy and healthy a country’s citizens are, but adjusts that by that country’s ecological footprint (how much “damage” the country does to planet Earth). Variables in the data set are:\n\n\nHPIRank, the rank of the country’s Happy Planet Index (lower is better)\n\nCountry, the name of the country\n\nLifeExpectancy, the average life expectancy of a citizen (in years)\n\nWellbeing, the average well being score (on a scale from 1 - 10). See the ladder question in the documentation for how this was calculated.\n\nHappyLifeYears, a combination of LifeExpectancy and Wellbeing\n\n\nFootprint, the ecological footprint per person (higher footprint means the average person in the country is less ecologically friendly)\n\nPopulation, the population size of the country\n\nGDPcapita, the Gross Domestic Product per population\n\nRegion, the region the country is in\n\nMake sure the hpi-tidy.csv data set is in a /data folder in your current R Project directory. Run the code in the following chunk to read in the data:\n\nlibrary(tidyverse)\nlibrary(here)\ntheme_set(theme_minimal())\nhpi_df &lt;- read_csv(here(\"data/hpi-tidy.csv\"))\nhpi_df\n#&gt; # A tibble: 151 × 11\n#&gt;   HPIRank Country     LifeExpectancy Wellbeing HappyLifeYears Footprint\n#&gt;     &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1     109 Afghanistan           48.7      4.76           29.0     0.540\n#&gt; 2      18 Albania               76.9      5.27           48.8     1.81 \n#&gt; 3      26 Algeria               73.1      5.24           46.2     1.65 \n#&gt; 4     127 Angola                51.1      4.21           28.2     0.891\n#&gt; 5      17 Argentina             75.9      6.44           55.0     2.71 \n#&gt; 6      53 Armenia               74.2      4.37           41.9     1.73 \n#&gt; # ℹ 145 more rows\n#&gt; # ℹ 5 more variables: HappyPlanetIndex &lt;dbl&gt;, Population &lt;dbl&gt;,\n#&gt; #   GDPcapita &lt;dbl&gt;, GovernanceRank &lt;chr&gt;, Region &lt;chr&gt;\n\n\n5.1.1 Making a Scatterplot and Labeling Points\nIn making visualizations, we sometimes want to either highlight, by using a unique colour, size, or shape, a specific observation or we might want to label a specific observation. The observation might be an outlying point, or it might just be a point of particular interest to your audience. For example, if you’re writing an article primarily for a United States reader audience using the hpi_df data set, you might want to label the United States because your readers presumably have an interest in the country that they live in.\nLet’s start by making a scatterplot of Footprint on the x-axis and Wellbeing on the y-axis to examine the relationship between countries’ carbon footprints and reported wellbeing of its citizens.\n\nggplot(data = hpi_df, aes(x = Footprint, y = Wellbeing)) +\n  geom_point()\n\n\n\n\nThere can be a few reasons we might want to label a point. We might be interested in a country out of personal interest or we might want to label any interesting outliers. For this example, let’s label the United States. First, we need to make a new data set that only has the United States of America observation. We can do this using filter() in the dplyr package, which only keeps certain rows, based on a condition that we provide.\n\nhpi_us &lt;- hpi_df |&gt; filter(Country == \"United States of America\")\nhpi_us\n#&gt; # A tibble: 1 × 11\n#&gt;   HPIRank Country           LifeExpectancy Wellbeing HappyLifeYears Footprint\n#&gt;     &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1     105 United States of…           78.5      7.16           61.3      7.19\n#&gt; # ℹ 5 more variables: HappyPlanetIndex &lt;dbl&gt;, Population &lt;dbl&gt;,\n#&gt; #   GDPcapita &lt;dbl&gt;, GovernanceRank &lt;chr&gt;, Region &lt;chr&gt;\n\nNow that we have this new data set, we can use it within geom_text(). Recall that the data = argument in ggplot() carries on through all geoms unless we specify otherwise. Now is our chance to “specify otherwise” by including another data = argument within geom_text():\n\nggplot(data = hpi_df, aes(x = Footprint, y = Wellbeing)) +\n  geom_point() +\n  geom_text(data = hpi_us, aes(label = Country)) ## specify\n\n\n\n## data = hpi_us so geom_text only uses the observation in hpi_us\n\nBut, it’s not that clear which of the points is actual the United States. A trick used in the R for Data Science book is to surround the points that are being labeled with an open circle using an extra geom_point() function to distinguish them and to use geom_text_repel() in the ggrepel library instead of geom_text() to “repel” the label away from the point that it’s labeling.\n\n## install.packages(\"ggrepel\")\nlibrary(ggrepel)\nggplot(data = hpi_df, aes(x = Footprint, y = Wellbeing)) +\n  geom_point() +\n  geom_text_repel(data = hpi_us, aes(label = Country)) +\n  geom_point(data = hpi_us, size = 3, shape = 1) ## create a second point that is an open circle (shape = 1) with a larger size (size = 3) to surround the United States point on the scatterplot\n\n\n\n\n\n5.1.2 Themes and Labels\nTo change labels and add titles to our ggplot graph, we can add + labs() to change various labels and titles throughout the plot:\n\nggplot(data = hpi_df, aes(x = Footprint, y = Wellbeing)) +\n  geom_point() +\n  geom_text_repel(data = hpi_us, aes(label = Country)) +\n  geom_point(data = hpi_us, size = 3, shape = 1) +\n  labs(title = \"Countries with a Higher Ecological Footprint Tend to Have Citizens with Higher Wellbeing\", ## add title\n    subtitle = \"Wellbeing is on a 1-10 scale\", ## add subtitle (smaller text size than the title)\n    caption = \"Data Source: http://happyplanetindex.org/countries\", ## add caption to the bottom of the figure\n    x = \"Ecological Footprint\", ## change x axis label\n    y = \"Wellbeing\") ## change y axis label\n\n\n\n\nMost labels can be changed through the a labs() layer addition.\nTo change a base theme of a plot, simply add + theme_name-of-theme() to your plot. Themes are listed at https://ggplot2.tidyverse.org/reference/ggtheme.html.\n\n\n\n\n\n\nNote\n\n\n\nWhich theme is best partially comes down to personal taste. My go-to is theme_minimal(): I love the clean look!\n\n\nThe purpose of the following set of exercises is for you to start reviewing some basic non-ggplot2 tidyverse functions from STAT/DATA 234. I will provide the sections that these functions were introduced in from my STAT/DATA 234 course, but feel free to use other sources! The link that I will reference is\nhttps://highamm.github.io/ds234_quarto/03-dplyr.html\nExercise 1: For each of the core dplyr functions mutate(), arrange(), select(), slice(), filter(), group_by(), and summarise(), write a one sentence summary of the overall purpose of the function.\nExercise 2: Review mutate(). Create a new variable in hpi_df that is equal to Wellbeing / Footprint.\nExercise 3: Review mutate() and case_when(). Create a new variable in hpi_df that is equal to \"80s\" if LifeExp is in the 80’s, \"70s\" if LifeExp is in the 70s, and \"Below 70\" if LifeExp is less than 70.\nExercise 4. Review arrange(). Sort the hpi_df data so that the country with the highest LifeExp is in the first row and the country with the lowest LifeExp is in the last row.\nExercise 5. Review select(). Make a data frame from hpi_df that only has the variables Country and LifeExp.\nExercise 6. Review slice(). Make a data frame from hpi_df that only has the first 5 rows.\nExercise 7. Review filter(). Make a data frame from hpi_df that only has countries in the \"Western World\" Region.\nExercise 8. Review filter(). Make a data frame from hpi_df that only has countries with a LifeExp less than 55.\nExercise 9. Review group_by() and summarise(). Make a table of the number of countries in each Region. (Hint: recall that the n() function is the counting function in dplyr).\nExercise 10. Review group_by() and summarise(). Make a table with the maximum LifeExp in each Region.\nAgain, we will see non-dplyr functions as well throughout the course, but we will try to review them as they come along. As long as you have a solid dplyr base (and, of course a good ggplot2 base), you should be good to go!"
  },
  {
    "objectID": "03-statdata234.html#more-review-with-billboard-hot-100",
    "href": "03-statdata234.html#more-review-with-billboard-hot-100",
    "title": "5  DATA/STAT 234 Review",
    "section": "\n5.2 More Review with Billboard Hot 100",
    "text": "5.2 More Review with Billboard Hot 100\nConsider a data set from the billboard R package, called wiki_hot_100s. To access the data set, we first need to install the billboard package:\n\n## install.packages(\"billboard\")\nlibrary(billboard)\nhead(wiki_hot_100s)\n#&gt;   no                     title              artist year\n#&gt; 1  1 Theme from A Summer Place         Percy Faith 1960\n#&gt; 2  2          He'll Have to Go          Jim Reeves 1960\n#&gt; 3  3             Cathy's Clown The Everly Brothers 1960\n#&gt; 4  4              Running Bear      Johnny Preston 1960\n#&gt; 5  5                Teen Angel        Mark Dinning 1960\n#&gt; 6  6                 I'm Sorry          Brenda Lee 1960\ntail(wiki_hot_100s)\n#&gt;       no                   title                             artist year\n#&gt; 5696  95 Adventure of a Lifetime                           Coldplay 2016\n#&gt; 5697  96         Humble and Kind                         Tim McGraw 2016\n#&gt; 5698  97                  Wicked                             Future 2016\n#&gt; 5699  98           Tiimmy Turner                          Desiigner 2016\n#&gt; 5700  99           See You Again Wiz Khalifa featuring Charlie Puth 2016\n#&gt; 5701 100                 Perfect                      One Direction 2016\n\nThe data set only contains a few variables:\n\n\nno, the number on the billboard year end Top 100 (1 - 100)\n\ntitle, the title of the song\n\nartist, the artist of the song\n\nyear, the year of the song\n\nOur first goal with this data set is to create a visualization that shows the most popular artists in the 2000s decade (2000 - 2009). We will use the number of songs the artist had that were in the top 100 throughout the decade as our metric for “popularity.”\n\n\n\n\n\n\nImportant\n\n\n\nIn creating a more complex data visualization, it is often most helpful to begin by drawing a sketch of the visualization that we want to end up with.\n\n\nWe will make a sketch as a class and then use our dplyr, ggplot2, and forcats knowledge to make the visualization in R.\nWe will complete this exercise as a class, so make sure to check out the Missed Class GitHub page if you miss class this day.\nExercise 1. Sketch out a visualization to show the most popular artists of the decade.\nExercise 2. Use dplyr, ggplot2, forcats, and other code to make this plot.\nExercise 3. There is a minor flaw in the way that we counted up the number of hits for each artist. Examine the 2nd to last row of the original data set with tail() to look at this potential flaw. What is this flaw?\nExercise 4. Fix the issue in the previous exercise.\nExercise 5. Use this website to customize the end points of your lollipop chart. If you have time, you can explore the other customization options. Make it look fancy!"
  },
  {
    "objectID": "03-statdata234.html#your-turn",
    "href": "03-statdata234.html#your-turn",
    "title": "5  DATA/STAT 234 Review",
    "section": "\n5.3 Your Turn",
    "text": "5.3 Your Turn\nThe 2000s is the decade of popular music that I am most familiar with, as it coincided with my middle school and high school years. However, many of you might be more interested in making this plot for the 2010s decade (2010 - 2019). But, note that the author of the billboard R package has not kept this data set up to date:\n\nmax(wiki_hot_100s$year)\n#&gt; [1] \"2016\"\n\nTherefore, to make this plot, we need a way to get the data from 2017, 2018, and 2019. Let’s first get the billboard hot 100 for a single year from Wikipedia : recall that we can use the rvest package to scrape a data table from a website into R. See the STAT 234 course materials for a refresher, though data scraping is not the most important topic at this point in the course. The following scrapes the billboard data from 2017 onward using an iterative approach with purrr.\n\n\n\n\n\n\nNote\n\n\n\nYou are not responsible for this code, but I left it in here in case you are interested (and, you will need to copy the code and run it to complete the exercise).\n\n\n\nlibrary(rvest)\nlibrary(tidyverse)\n\n## function to scrape data from wikipedia for a particular year\nget_wiki_100 &lt;- function(year) {\n  \n  ## same code as before, replacing 2017 with year.\n  url &lt;- paste0(\"https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_\", year)\n  \n  h &lt;- read_html(url)\n  \n  tab &lt;- h |&gt; html_nodes(\"table\")\n  df &lt;- tab[[1]] |&gt; html_table() |&gt;\n    mutate(year = year)\n  \n  ## tell our function to return the dataframe `df`\n  return(df) \n}\n\n## apply the function to the following years\nlibrary(purrr)\nyear_list &lt;- list(2017, 2018, 2019, 2020, 2021)\n\ndf_all &lt;- purrr::map(year_list, get_wiki_100)\n\n## combine the data frames\ndf_2017_present &lt;- bind_rows(df_all)\ndf_2017_present &lt;- df_2017_present |&gt;\n  mutate(Title = str_remove_all(Title, pattern = \"\\\"\")) |&gt; ## get rid of \\ in title\n  rename(no = No., \n         title = Title, \n         artist = `Artist(s)`) ## make column names match with billboard package\n\nwiki_tibble &lt;- as_tibble(wiki_hot_100s) |&gt; ## convert billboard data to tibble\n  mutate(year = as.numeric(year),\n         no = as.integer(no)) ## change variable types to match with scraped data\n\nhot100_df &lt;- bind_rows(wiki_tibble, df_2017_present)\nhot100_df\n#&gt; # A tibble: 6,201 × 4\n#&gt;      no title                     artist               year\n#&gt;   &lt;int&gt; &lt;chr&gt;                     &lt;chr&gt;               &lt;dbl&gt;\n#&gt; 1     1 Theme from A Summer Place Percy Faith          1960\n#&gt; 2     2 He'll Have to Go          Jim Reeves           1960\n#&gt; 3     3 Cathy's Clown             The Everly Brothers  1960\n#&gt; 4     4 Running Bear              Johnny Preston       1960\n#&gt; 5     5 Teen Angel                Mark Dinning         1960\n#&gt; 6     6 I'm Sorry                 Brenda Lee           1960\n#&gt; # ℹ 6,195 more rows\n\nExercise 1. Use the hot100_df to make either a lollipop chart of the most popular artists of the 2010s (2010 through 2019). Important: It is easy to make this plot by copy-pasting the code we used in class and just changing the years used. However, for a better review, you should try to make this plot without looking back at the code for the 2000s plot until you get stuck.\nExercise 2. You may have used a SLU majors data set in DATA/STAT 234, but now, we will use a data set that is a little different in that it shows the major/minor combination of each student. That is, instead of each row being a major, each row is an individual student. This allows us to look at more detailed questions about what students tend to double-major or minor in. The data was obtained from Christine Zimmerman in SLU’s Institutional Research Office and contains data on all graduating students in the past 7 years.\nThe variables in the data set are:\n\n\nadm_id, a unique ID assigned to each student,\n\nsex, the sex of the student,\n\nmajor1, major2, major3, minor1, minor2, minor3, the majors and minors of the student.\n\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(here)\ndf &lt;- read_excel(here::here(\"data/slu_graduates_17_23.xlsx\"))\n\n## fixes error in the data\ndf &lt;- df |&gt; mutate(across(everything(),\n                          .fns = ~replace(., . ==  \"STATS\" , \"STAT\")))\n\nUsing this data set, construct a visualization that shows the proportion of students within each major who identify as female. We will begin this exercise by sketching out what we want to create as a class."
  },
  {
    "objectID": "06-mapping.html#mapping-introduction-class-prep",
    "href": "06-mapping.html#mapping-introduction-class-prep",
    "title": "6  Mapping Data",
    "section": "\n6.1 Mapping Introduction (Class Prep)",
    "text": "6.1 Mapping Introduction (Class Prep)\n\n6.1.1 Do We Need a Map?\n\n\n\n\n\n\nImportant\n\n\n\nJust because your data is spatial does not mean the best way to display the data is a map.\n\n\nThis is the most important take-away from this subsection. Section 7.5 of the data visualization textbook that we used to help conceptualize data visualization principles has a great section on this. Read this section. There are two exercises that are given after the following “Mapping Common Areas” subsection pertaining to this reading.\n\n6.1.2 Mapping Common Areas\nIn order to create a map of some area, we need some way to tell R how to draw the boundaries for that area. This may seem trivial, but, in many cases, it isn’t because boundaries in maps are often complex. Think of the state boundaries in the United States: some boundaries are straight lines, while others are much more complex. Some areas, like the United States, are commonly mapped by many data scientists. The maps package contains pre-“drawn” maps for many of these areas. We can see in (this link that some of these areas include a map of the world, a map of lakes, a map of france, a map of us.cities, and a map of the state data in the United States.\n\n## install.packages(\"maps\")\nlibrary(maps)\nlibrary(tidyverse)\nstate_df &lt;- ggplot2::map_data(\"state\")\n\nThe map_data() function returns a data.frame object, but this data frame has many observations (15537) instead of just 50. Why? Because it contains all of the information necessary for R to draw the polygonal shapes of each state. Variables in the data frame include\n\n\nlong and lat, latitude and longitude\n\ngroup, one number for each polygon R must draw\n\norder, the order that R will draw the line segments within a group to form the polygon\n\nregion, the name of the state\n\nTo have R actually draw the map, we just need a new geom: geom_polygon() and a group aes() to tell R how to draw the boundaries:\n\nggplot(data = state_df,\n            mapping = aes(x = long, y = lat,\n                          group = group)) +\n  geom_polygon() \n\n\n\n\nYou may notice some not-so-great things about our default map. The aspect ratio seems off, the states are already filled in as black, and the plot has unnecessary gridlines.\nThe following is code to “fix” some issues with the initial map of the United States. The theme_void() theme gets rid of the axis gridlines, axis ticks, and axis text. The colour and fill arguments to geom_polygon() give states a black outline colour and a fill of white. The most novel part of the code is the coord_map() function. This lets us specify a particular projection to use.\n\nggplot(data = state_df,\n            mapping = aes(x = long, y = lat,\n                          group = group)) +\n  geom_polygon(colour = \"black\", fill = \"white\") +\n  coord_map(projection = \"albers\", lat0 = 39, lat1 = 45) +\n  theme_void()\n\n\n\n\nWe used an \"albers\" projection to make the map above, which is a commonly used projection for the Untied States. Note that the choice of projection matters, particularly for maps that cover larger areas of the globe. The video found here https://www.youtube.com/watch?v=kIID5FDi2JQ explains more about projections. We will watch the video in class, so there is no need for you to watch this video on your own.\n\n\n\n\n\n\nNote\n\n\n\nWe unfortunately do not have time to talk about map projection in more detail, but, this is an area you would want to read more about if your final project involves mapping of some kind. Or, if you are interested, you can read more about projections at these two links: https://medium.com/nightingale/understanding-map-projections-8b23ecbd2a2f or https://mdsr-book.github.io/mdsr2e/ch-spatial.html#sec:projections.\n\n\nNow that we’ve created a base map, we probably want to add a variable to fill by to examine patterns across the states. The resulting map is sometimes called a “choropleth” map. We will use the state_stats data set from the usdata package.\n\n## install.packages(\"usdata\")\nlibrary(usdata)\nstate_stats\n#&gt; # A tibble: 51 × 24\n#&gt;   state abbr   fips pop2010 pop2000 homeownership multiunit income med_income\n#&gt;   &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 Alab… AL        1  4.78e6  4.45e6          71.1      15.5  22984      42081\n#&gt; 2 Alas… AK        2  7.10e5  6.27e5          64.7      24.6  30726      66521\n#&gt; 3 Ariz… AZ        4  6.39e6  5.13e6          67.4      20.7  25680      50448\n#&gt; 4 Arka… AR        5  2.92e6  2.67e6          67.7      15.2  21274      39267\n#&gt; 5 Cali… CA        6  3.73e7  3.39e7          57.4      30.7  29188      60883\n#&gt; 6 Colo… CO        8  5.03e6  4.30e6          67.6      25.6  30151      56456\n#&gt; # ℹ 45 more rows\n#&gt; # ℹ 15 more variables: poverty &lt;dbl&gt;, fed_spend &lt;dbl&gt;, land_area &lt;dbl&gt;,\n#&gt; #   smoke &lt;dbl&gt;, murder &lt;dbl&gt;, robbery &lt;dbl&gt;, agg_assault &lt;dbl&gt;,\n#&gt; #   larceny &lt;dbl&gt;, motor_theft &lt;dbl&gt;, soc_sec &lt;dbl&gt;, nuclear &lt;dbl&gt;,\n#&gt; #   coal &lt;dbl&gt;, tr_deaths &lt;dbl&gt;, tr_deaths_no_alc &lt;dbl&gt;, unempl &lt;dbl&gt;\n\nYou can read about each of the variables with ?state_stats.\nThere really is not too much new information here: we use the dplyr join functions to combine the two data frames. We then often simply use a fill aes() within geom_polygon() to fill by the variable of interest and scale_fill_viridis() to change the fill scale.\n\nstate_stats &lt;- state_stats |&gt; mutate(state = str_to_lower(state))\nstate_full &lt;- left_join(state_df, state_stats, by = c(\"region\" = \"state\"))\n\nggplot(data = state_full, aes(x = long, y = lat, group = group)) +\n  geom_polygon(colour = \"black\", aes(fill = coal)) +\n  coord_map(projection = \"albers\", lat0 = 39, lat1 = 45) +\n  theme_void() +\n  scale_fill_viridis_b()\n\n\n\n\nExercise 1. The reading gave one example where the “best” graph to show woud not be a map. Think of a second example where, even though the data is spatial, the best graph to make a particular argument would not be a map.\nExercise 2. Refer back the United States examples that we completed. Choose a variable or two variables where a map makes the most sense to visualize that variable and explain your reasoning. Then, choose a variable or two variables where you might make a plot other than a map and explain your reasoning for why the map makes less sense for this variable.\nExercise 3. Which states had the fastest growth rate between 2000 and 2010? Make a variable for the percent change in population and then map this variable.\nExercise 4. To your map in Exercise 1, think about which type of colour scale makes the most sense to use (sequential, diverging, or unordered). Change the colour scale to match what makes the most sense.\nYou may have noticed a couple of additional problems in the United States maps we made in the previous section.\n\nWe ignored Alaska and Hawaii. Putting these on the map in their geographic locations would result in a tiny continental United States map that would be harder to read.\nSome states in the Northeast are so small that their fill values are hard to read.\n\nBoth of these issues can be challenging to address. It is common to put Hawaii and Alaska in a corner of the plot, even though their latitude and longitude values do not match. Sometimes Alaska is made to be smaller than its true size as well. The smaller Northeastern states is a more challenging problem. This is beyond the time we have to devote to maps in this course, but a hexbin map can help with both of these issues."
  },
  {
    "objectID": "06-mapping.html#mapping-other-areas-sf-objects",
    "href": "06-mapping.html#mapping-other-areas-sf-objects",
    "title": "6  Mapping Data",
    "section": "\n6.2 Mapping Other Areas: sf Objects",
    "text": "6.2 Mapping Other Areas: sf Objects\nThe built-in R map data and the data from the maps package is really nice for mapping common areas like the world and the United States. However, sometimes we may want to construct a map that does not have a corresponding data set in base R or maps. Most commonly, we use an sf object for this purpose. In this section, we will construct a map of the Covid cases in St. Lawrence county, borrowing some code and data from Dr. Ramler.\nBefore we deal with the map, let’s first modify a data set provided by Dr. Ramler that has active and total numbers of Covid cases in St. Lawrence County (SLC). The description of the code is omitted but almost everything is something you would have seen in STAT/DATA 234.\n\nactive &lt;- read_csv(\"https://raw.githubusercontent.com/iramler/stlawu_covid/main/slc_towns_active_cases.csv\", n_max = 34)\ntcases &lt;- read_csv(\"https://raw.githubusercontent.com/iramler/stlawu_covid/main/slc_towns_total_cases.csv\", n_max = 34)\n\nactive_long &lt;- active |&gt; pivot_longer(5:ncol(active), names_to = \"date\",\n                                       values_to = \"cases\")\n## repeat for total cases\ntcases_long &lt;- tcases |&gt; pivot_longer(5:ncol(tcases), names_to = \"date\",\n                                       values_to = \"cases\")\n\nlibrary(lubridate)\ncovid_df &lt;- left_join(tcases_long, active_long,\n                      by = c(\"date\", \"Order\", \"NAME\")) |&gt;\n  mutate(date = mdy(date)) |&gt;\n  rename(total_cases = cases.x,\n         active_cases = cases.y) |&gt;\n  mutate(total_cases = if_else(is.na(total_cases),\n                               true = 0, false = total_cases),\n         active_cases = if_else(is.na(active_cases),\n                                      true = 0, false = active_cases))\n  \n\ncovid_SLC &lt;- covid_df |&gt; filter(NAME == \"St. Lawrence County\")\ncovid_sub &lt;- covid_df |&gt; filter(NAME != \"St. Lawrence County\")\n\nWe now have two data frames, covid_SLC, which contains cases for St. Lawrence County as a whole, and covid_sub, which has some information on Covid cases in the various sub-regions of SLC. Important variables include:\n\n\nNAME, the name of the county sub-area,\n\nPopulation.x, the county population,\n\ndate, the date\n\ntotal_cases, the total number of cases\n\nactive_cases, the number of active cases\n\nThere is plenty we can do with this data set without mapping.\nExercise 1. Make a line plot that shows the number of active_cases in all of St. Lawrence County over time.\nBut suppose that we now want to make a map of the current day’s active cases in the subregions of SLC like the maps found here . We need to tell R how to draw the lines that define the different subregions of SLC. To do so, we need to provide R with a shapefile that has directions on how to draw the spatial polygons.\nHow can we obtain this shapefile? There are two primary methods:\n\nThrough GIS. You can export a shapefile for a particular region using GIS software, something I am not that familiar with. But, Carol Cady at SLU has GIS expertise.\nSearching the Internet. In this case, Dr. Ramler found a shape file for the subregions of all counties in New York at this website. He and Caroline Krall, a student, then had to subset the data there to only contain information for the subregions in St. Lawrence County.\n\n\n\n\n\n\n\nImportant\n\n\n\nA shapefile typically has an extension of .shp. It needs to be accompanied by three other “helper” files: a .dbf file, a prj file, and a shx file.\n\n\nThe most popular package to work with shapefiles is the sf package, which stands for simple features.\n\n## install.packages(\"sf\")\nlibrary(sf)\nshp &lt;- read_sf(\"data/SLC_Civil_Boundaries_SHP/slc.shp\") |&gt;\n  st_transform(st_crs(\"+proj=longlat\"))\n\nNote that we need to supply a coordinate reference system (CRS) with the st_crs() function to change the coordinates to latitude/longitude.\nThe object shp is a simple features object and contains not only the data but also the information R needs to draw the polygons. We can run str(shp) to examine its structure. We can also construct a map by using a geom particular to sf objects: geom_sf():\n\nggplot(data = shp) +\n  geom_sf() +\n  theme_void()\n\n\n\n\nFinally, we need to use our joining functions to combine the data from shp with the covid data from covid_sub. sf are the most popular objects to store spatial information because they are mostly compatible with the tidyverse functions we already know and love. So, a left_join() does the trick here:\n\nfull_df &lt;- left_join(shp, covid_sub, by = \"NAME\") |&gt;\n  filter(date == max(date)) ## only plot cases on the most recent date\n\nWe then make a plot with:\n\nggplot(data = full_df) +\n  geom_sf(aes(fill = active_cases)) +\n  theme_void()\n\n\n\n\nExercise 2. Change the fill scale of the plot. Should you use an unordered, sequential, or diverging scale?\nExercise 3. Change the colour scale so that active_cases are put into different bins with scale_fill_viridis_b(). What are some advantages and disadvantages of this?\nExercise 4. Explore the ?geom_sf_text() function and add the actual number of cases to the subregions in the plot, as is done on the SLC website."
  },
  {
    "objectID": "06-mapping.html#your-turn",
    "href": "06-mapping.html#your-turn",
    "title": "6  Mapping Data",
    "section": "\n6.3 Your Turn",
    "text": "6.3 Your Turn\nConsider again the Happy Planet Index data. Recall that the basic idea of a “happy planet index” is that it should be a metric that computes how happy and healthy a country’s citizens are, but adjusts that by that country’s ecological footprint (how much “damage” the country does to planet Earth). Variables in the data set are:\n\n\nHPIRank, the rank of the country’s Happy Planet Index (lower is better)\n\nCountry, the name of the country\n\nLifeExpectancy, the average life expectancy of a citizen (in years)\n\nWellbeing, the average well being score (on a scale from 1 - 10).\n\nHappyLifeYears, a combination of LifeExpectancy and Wellbeing\n\nFootprint, the ecological footprint per person (higher footprint means the average person in the country is less ecologically friendly)\n\nPopulation, the population size of the country\n\nGDPcapita, the Gross Domestic Product per population\n\nRegion, the region the country is in\n\n\nlibrary(tidyverse)\nlibrary(here)\nhpi_df &lt;- read_csv(here(\"data/hpi-tidy.csv\"))\n\nExericse 1. Make a map of a variable of your choosing. In coord_map(), use projection = \"mercator\", which is also the default (we will see in a later exercise that this probably is not the best choice).\nHint: in ggplot2’s map_data() function, there is a built in map of the \"world\".\nHint: You can read more about projections in Section 17.3.2 of Modern Data Science with R\nExercise 2. You may notice that the United States does not get coloured in your map. Examine this issue further and fix the map so that the United States is coloured.\nExercise 3. You may have noticed that there are two horizontal stripes across your map. This is an issue that drove me nuts! Check out this submitted issue on ggplot2’s GitHub page for the reason for the error as well as a fix. Use it to fix your plot.\nExercise 4. Read about Mercator projections in this blog post. What does this source say about the sizes of Greenland vs. Africa in a Mercator projection.\nExercise 5. Examine all of the different options for map projection with ?mapproject. Then, change the projection to \"globular\". Change the projection again to \"gilbert\". How does the relative size of Greenland to Africa change in the projections?\n\n\n\n\n\n\nNote\n\n\n\nYou can read more about projections in R at this site. There’s definitely a lot going on, and you should always consider what map projection you are using when mapping data, especially if that data is on a global scale!"
  },
  {
    "objectID": "05-uncertainty.html#are-bar-plots-bad-class-prep",
    "href": "05-uncertainty.html#are-bar-plots-bad-class-prep",
    "title": "7  Expressing Variability/Uncertainty",
    "section": "\n7.1 Are Bar Plots Bad? (Class Prep)",
    "text": "7.1 Are Bar Plots Bad? (Class Prep)\nRead this short article from the Nature scientific journal, arguing that bar charts “should be purged from much of the scientific literature.”\nBut, we’ve constructed a few bar plots (and lollipop plots) throughout this course. Have we been making a major mistake?\nIn some contexts, bar plots are very bad. In general, when they are used to represent summary statistics from continuous variables (like a plot of the sample mean for each level of a categorical variable), they can obscure a lot of the underlying data’s relevant features, including the distribution shape and the sample size. But perhaps most importantly, they obscure any variability in the data. We will see some examples of this in the examples below.\nWe have used bar plots or lollipop plots to graph counts of categorical variables. In general, using bar plots to represent counts or proportions is still perfectly fine!\nI would be remiss if I did not include the Pokemon data set at least once in this course. So, here it is! Suppose that we want to compare heights of a few different types of pokemon: Bug, Electric, Fighting, Flying, Grass, and Steel pokemon. To do so, we will make a bar plot of the average height of each type with the code below:\n\nlibrary(tidyverse)\ntheme_set(theme_minimal())\n\nlibrary(here)\npokemon_df &lt;- read_csv(here(\"data/pokemon_full.csv\"))\npokemon_height &lt;- pokemon_df |&gt; \n  filter(Type %in% c(\"Bug\", \"Electric\", \"Fighting\", \"Flying\",\n                     \"Grass\", \"Steel\")) |&gt;\n  group_by(Type) |&gt;\n  summarise(avg_height = mean(height)) |&gt;\n  mutate(Type = fct_reorder(Type, avg_height))\n\nggplot(data = pokemon_height, aes(x = Type, y = avg_height)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\nFrom this graph, it might be tempting to conclude that, on average, Steel type pokemon are the tallest while the other types have approximately equal average height. In particular, it looks like Steel type Pokemon have twice the height of Bug type Pokemon, on average.\nExercise 1. What can’t we see from this graphic that would be useful?\nExercise 2. Make a different plot that shows more relevant features about the distribution of the height variable in each Pokemon type.\nThis issue is very easy to find in other real data sets. As a second example, consider a baseball data set that graphs the median number of homeruns for each player position from a set of Major League Baseball players in the year 2018.\n\n## install.packages(\"openintro\")\nlibrary(openintro)\ndata(mlb_players_18)\nmlb_sum &lt;- mlb_players_18 |&gt; group_by(position) |&gt;\n  summarise(med_hr = median(HR)) |&gt;\n  mutate(position = fct_reorder(position, med_hr))\nggplot(data = mlb_sum, aes(x = position, y = med_hr)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\nExercise 3. “Fix” the previous plot to show the underlying variability in the number of homeruns for each player position by making a set of boxplots.\nExercise 4.\nConsider a news channel covering a developing hurricane. Which of these types of graphs would better help the general public with the potential variability of the hurricane’s path?\nOption 1\nOR\nOption 2\nExercise 5.\nNext, consider fivethirtyeight.com’s coverage of the 2020 presidential election. Much of their forecast given on this page can be simply summarised by saying they predict Biden to win the election with 89% probability. So, why make the supplementary graphics that say the same thing but use a lot more space?"
  },
  {
    "objectID": "05-uncertainty.html#stat-113-survey",
    "href": "05-uncertainty.html#stat-113-survey",
    "title": "7  Expressing Variability/Uncertainty",
    "section": "\n7.2 STAT 113 Survey",
    "text": "7.2 STAT 113 Survey\nYou may recall from taking STAT 113 that you completed a survey at the beginning of the semester. Responses from the survey were then used in some examples throughout the semester. We will explore some of these surveys from the past few years, examining different trends in some of the variables through time. For this example, we will also pay particular attention to making sure that our graphics show the underlying variability in the data.\nTo begin, read in the data set:\n\nlibrary(here)\nstatsurvey_df &lt;- read_csv(here(\"data/stat113_survey.csv\"))\n\nThe data set has many variables. Some that we will use are:\n\n\ntime_both, the semester and year the survey was taken\n\nGPA, the current GPA of the student\n\nTattoo, whether or not the student has a Tattoo\n\nTV, the amount of time spent watching TV per week\n\nFacebook, the number of Facebook friends\n\nUsing this data set, we answer some interesting questions. First, is there evidence of grade inflation at SLU? That is, is there evidence that student GPAs have increased over time? We will start this problem by making a sketch of a couple of visualizations we might want to create."
  },
  {
    "objectID": "05-uncertainty.html#your-turn",
    "href": "05-uncertainty.html#your-turn",
    "title": "7  Expressing Variability/Uncertainty",
    "section": "\n7.3 Your Turn",
    "text": "7.3 Your Turn\nExercise 1. Is there evidence from the STAT 113 survey that tattoos have become more or less common (at least among SLU students)? Construct a plot that shows the proportion of students who have a Tattoo in each semester from the STAT 113 survey, along with standard error bars for the estimate in each semester (calculated using the formula \\(SE = \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\\))."
  },
  {
    "objectID": "07-stat213.html#stat-213-with-broom-class-prep",
    "href": "07-stat213.html#stat-213-with-broom-class-prep",
    "title": "8  STAT 213 Review",
    "section": "\n8.1 STAT 213 with broom (Class Prep)",
    "text": "8.1 STAT 213 with broom (Class Prep)\nIn this section, we will focus more on a tidy approach to the models that you fit and interpreted in STAT 213.\n\n\n\n\n\n\nNote\n\n\n\nYou may or may not have completed STAT 213 using the functions from the broom package (tidy(), augment(), and glance()), but this section will assume that you have not used these functions.\n\n\nThe functions in the broom package return tibbles with model summary information that we can then use for further analysis, plotting, or presentation. The broom package consists of three primary functions: tidy(), glance(), and augment(), which are described below.\nWe will use the coffee_ratings data set, which contains observations on ratings of various coffees throughout the world. The data was obtained from the Github account (https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-07-07/readme.md).\nA description of each variable in the data set is given below.\n\n\ntotal_cup_points, the score of the coffee by a panel of experts (our response variable for this section)\n\nspecies, the species of the coffee bean (Arabica or Robusta)\n\naroma, aroma (smell) grade\n\nflavor, flavor grade\n\naftertaste, aftertaste grade\n\nacidity, acidity grade\n\nbody, body grade\n\nbalance, balance grade\n\nuniformity, uniformity grade\n\nclean_cup, clean cup grade\n\nsweetness, sweetness grade\n\nmoisture, moisture grade\n\ncategory_one_defects, count of category one defects\n\nquakers, quakers\n\ncategory_two_defects, the number of category two defects\n\nIn the examples below, we will consider the multiple linear regression model\n\\[\nY = \\beta_0 + \\beta_1 species + \\beta_2 aroma + \\beta_3 flavor + \\beta_4 sweetness + \\beta_5 moisture + \\epsilon,\n\\]\nwhere \\(Y\\) is the rating of the coffee (total_cup_points), species is an indicator variable equal to 1 if the species is Robusta and a 0 if the species is Arabica, and \\(epsilon\\) are the random errors, which follow the usual assumptions that they are independent and normally distributed with constant variance.\n\n8.1.1 tidy()\n\ntidy() is analagous to summary() for a linear model object. Let’s start by fitting a linear model with lm() with total_cup_points as the response and species, aroma, flavor, sweetness, and moisture as predictors.\nRead in the data, load the broom package (and install it with install.packages(\"broom\")), and fit the model with\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(here)\ntheme_set(theme_minimal())\n\ncoffee_df &lt;- read_csv(here(\"data/coffee_ratings.csv\"))\ncoffee_mod &lt;- lm(total_cup_points ~ species + aroma + flavor +\n                   sweetness + moisture,\n   data = coffee_df)\n\nIn STAT 213, you likely used summary() to look at the model output:\n\nsummary(coffee_mod)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = total_cup_points ~ species + aroma + flavor + sweetness + \n#&gt;     moisture, data = coffee_df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -9.5132 -0.3705  0.0726  0.5610  5.5844 \n#&gt; \n#&gt; Coefficients:\n#&gt;                Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)     7.04039    0.77377   9.099  &lt; 2e-16 ***\n#&gt; speciesRobusta  2.85365    0.26861  10.624  &lt; 2e-16 ***\n#&gt; aroma           1.95188    0.14575  13.392  &lt; 2e-16 ***\n#&gt; flavor          5.09440    0.14042  36.281  &lt; 2e-16 ***\n#&gt; sweetness       2.23956    0.06553  34.173  &lt; 2e-16 ***\n#&gt; moisture       -1.88033    0.67368  -2.791  0.00533 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.168 on 1333 degrees of freedom\n#&gt; Multiple R-squared:  0.8891, Adjusted R-squared:  0.8887 \n#&gt; F-statistic:  2137 on 5 and 1333 DF,  p-value: &lt; 2.2e-16\n\nHowever, there are a few inconveniences involving summary(). First, it’s just not that nice to look at: the output isn’t formatted in a way that is easy to look at. Second, it can be challenging to pull items from the summary output with code. For example, if you want to pull the p-value for moisture, you would need to write something like:\n\nsummary(coffee_mod)$coefficients[\"moisture\", 4]\n#&gt; [1] 0.005327594\n\ntidy() is an alternative that puts the model coefficients, standard errors, t-stats, and p-values in a tidy tibble:\n\ntidy(coffee_mod)\n#&gt; # A tibble: 6 × 5\n#&gt;   term           estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)        7.04    0.774       9.10 3.23e- 19\n#&gt; 2 speciesRobusta     2.85    0.269      10.6  2.31e- 25\n#&gt; 3 aroma              1.95    0.146      13.4  1.82e- 38\n#&gt; 4 flavor             5.09    0.140      36.3  4.73e-201\n#&gt; 5 sweetness          2.24    0.0655     34.2  2.41e-184\n#&gt; 6 moisture          -1.88    0.674      -2.79 5.33e-  3\n\nThe advantage of this format of output is that we can now use other tidyverse functions on the output. To pull the p-values,\n\ntidy(coffee_mod) |&gt; select(p.value)\n#&gt; # A tibble: 6 × 1\n#&gt;     p.value\n#&gt;       &lt;dbl&gt;\n#&gt; 1 3.23e- 19\n#&gt; 2 2.31e- 25\n#&gt; 3 1.82e- 38\n#&gt; 4 4.73e-201\n#&gt; 5 2.41e-184\n#&gt; 6 5.33e-  3\n\nor, to grab the output for a particular variable of interest:\n\ntidy(coffee_mod) |&gt; filter(term == \"aroma\")\n#&gt; # A tibble: 1 × 5\n#&gt;   term  estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 aroma     1.95     0.146      13.4 1.82e-38\n\n\n8.1.2 glance()\n\nglance() puts some model summary statistics into a tidy tibble. For example, if we run\n\nglance(coffee_mod)\n#&gt; # A tibble: 1 × 12\n#&gt;   r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n#&gt;       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     0.889         0.889  1.17     2137.       0     5 -2105. 4224. 4260.\n#&gt; # ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nyou should notice a lot of statistics that you are familiar with from STAT 213, including r.squared, adj.r.squared, sigma (the residual standard error), statistic (the overall F-statistic), AIC, and BIC. We are going to focus mainly on coefficient interpretation in this course, so there is no need to worry if you do not remember exactly what, for example, BIC is.\n\n8.1.3 augment()\n\naugment() is my personal favourite of the three. The function returns a tibble that contains all of the variables used to fit the model appended with commonly used diagnostic statistics like the fitted values (.fitted), cook’s distance (.cooksd), .hat values for leverage, and residuals (.resid).\n\naugment(coffee_mod)\n#&gt; # A tibble: 1,339 × 12\n#&gt;   total_cup_points species aroma flavor sweetness moisture .fitted  .resid\n#&gt;              &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1             90.6 Arabica  8.67   8.83        10     0.12    91.1 -0.537 \n#&gt; 2             89.9 Arabica  8.75   8.67        10     0.12    90.5 -0.538 \n#&gt; 3             89.8 Arabica  8.42   8.5         10     0       89.2  0.577 \n#&gt; 4             89   Arabica  8.17   8.58        10     0.11    88.9  0.114 \n#&gt; 5             88.8 Arabica  8.25   8.5         10     0.12    88.6  0.214 \n#&gt; 6             88.8 Arabica  8.58   8.42        10     0.11    88.9 -0.0411\n#&gt; # ℹ 1,333 more rows\n#&gt; # ℹ 4 more variables: .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;,\n#&gt; #   .std.resid &lt;dbl&gt;\n\naugment() the data set makes it really easy to do things like:\n\n\nfilter() the data set to examine values with high cook’s distance that might be influential\n\n\naugment_df &lt;- augment(coffee_mod)\naugment_df |&gt; filter(.cooksd &gt; 1)\n#&gt; # A tibble: 1 × 12\n#&gt;   total_cup_points species aroma flavor sweetness moisture .fitted .resid\n#&gt;              &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1                0 Arabica     0      0         0     0.12    6.81  -6.81\n#&gt; # ℹ 4 more variables: .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;,\n#&gt; #   .std.resid &lt;dbl&gt;\n\nWe see right away that there is a potentially influential observation with 0 total_cup_points. Examining this variable further, we see that it is probably a data entry error that can be removed from the data.\n\nggplot(data = coffee_df, aes(x = total_cup_points)) +\n  geom_histogram(bins = 15, fill = \"white\", colour = \"black\")\n\n\n\n\nWe could also find observations with high leverage\n\naugment_df |&gt; filter(.hat &gt; 0.2)\n#&gt; # A tibble: 2 × 12\n#&gt;   total_cup_points species aroma flavor sweetness moisture .fitted .resid\n#&gt;              &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1             59.8 Arabica   7.5   6.67      1.33     0.1    58.4    1.38\n#&gt; 2              0   Arabica   0     0         0        0.12    6.81  -6.81\n#&gt; # ℹ 4 more variables: .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;,\n#&gt; #   .std.resid &lt;dbl&gt;\n\nor observations that are outliers:\n\naugment_df |&gt; filter(.std.resid &gt; 3 | .std.resid &lt; -3)\n#&gt; # A tibble: 25 × 12\n#&gt;   total_cup_points species aroma flavor sweetness moisture .fitted .resid\n#&gt;              &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1             82.8 Arabica  8.08   8.17     10        0.12    86.6  -3.85\n#&gt; 2             82.4 Arabica  5.08   7.75     10        0.11    78.6   3.79\n#&gt; 3             82.3 Arabica  7.75   8.08      6.67     0.11    78.1   4.27\n#&gt; 4             80.7 Arabica  7.67   7.5       6.67     0       75.2   5.51\n#&gt; 5             80   Arabica  7.58   7.75     10        0       83.7  -3.71\n#&gt; 6             79.9 Arabica  7.83   7.67     10        0       83.8  -3.87\n#&gt; # ℹ 19 more rows\n#&gt; # ℹ 4 more variables: .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;,\n#&gt; #   .std.resid &lt;dbl&gt;\n\nFinally, we can use our ggplot2 skills to construct plots like a residuals versus fitted values plot (filtering out the outlying observation first):\n\nggplot(data = augment_df |&gt; filter(.fitted &gt; 25),\n       aes(x = .fitted, y = .resid)) +\n  geom_point() \n\n\n\n\n\n8.1.4 Exercises\nExercise 1. Examine the penguins data set in the palmerpenguins package:\n\nlibrary(palmerpenguins)\npenguins\n#&gt; # A tibble: 344 × 8\n#&gt;   species island   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#&gt;   &lt;fct&gt;   &lt;fct&gt;             &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n#&gt; 1 Adelie  Torgers…           39.1          18.7               181        3750\n#&gt; 2 Adelie  Torgers…           39.5          17.4               186        3800\n#&gt; 3 Adelie  Torgers…           40.3          18                 195        3250\n#&gt; 4 Adelie  Torgers…           NA            NA                  NA          NA\n#&gt; 5 Adelie  Torgers…           36.7          19.3               193        3450\n#&gt; 6 Adelie  Torgers…           39.3          20.6               190        3650\n#&gt; # ℹ 338 more rows\n#&gt; # ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\nFit a linear regression model with body_mass_g as the response variable and species and bill_length_mm as the predictors. Note that penguins with missing values for any of these three variables will be dropped from the analysis.\nExercise 2. Create a table of summary output, including coefficient estimates, standard errors, test statistics, and p-values, using one of the broom functions.\nExercise 3. Use glance() to glance at some of the relevant model statistics.\nExercise 4. Using augment(), create a plot of the residuals vs. the fitted values and evaluate the constant variance assumption.\nExercise 5. Using augment(), check to see if there are any penguins that are influential. Use 0.75 as your cut-off value for Cook’s distance."
  },
  {
    "objectID": "07-stat213.html#model-coefficient-interpretation",
    "href": "07-stat213.html#model-coefficient-interpretation",
    "title": "8  STAT 213 Review",
    "section": "\n8.2 Model Coefficient Interpretation",
    "text": "8.2 Model Coefficient Interpretation\nThe goal of this section is to review intercept and slope coefficient interpretations from STAT 213. Our ultimate goal, in the coming weeks, is to use visualization to make our model coefficient interpretation more clear. We will have a particular focus on interpreting model coefficients from relatively complex models (with interaction terms, for example) for an audience with little statistical background.\nWe will use an accompanying handout and pen-and-paper to review model coefficient interpretation from STAT 213."
  },
  {
    "objectID": "08-model-interactions.html#basic-strategy-class-prep",
    "href": "08-model-interactions.html#basic-strategy-class-prep",
    "title": "9  Linear Model Visualization",
    "section": "\n9.1 Basic Strategy (Class Prep)",
    "text": "9.1 Basic Strategy (Class Prep)\nOur basic strategy for visualizing models is to\n\nfit the model of interest with lm().\nconstruct a grid of predictor values with the data_grid() function from the modelr package.\nUse the augment() function from the broom package on the data grid in (2) to predict the response variable according to the model for each row in the grid.\nUse ggplot2 to construct a meaningful plot with the model predictions from (3).\n\nWe will begin by fitting a linear regression model with score as the response and age (in years) as the predictor. Note that we can easily visualize this model because of how simple it is:\n\nggplot(data = evals, aes(x = age, y = score)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\nOur first goal is to recreate the plot above “by hand” so that we can see what the different functions are doing in a relatively simple example.\nStep 1: Fit the model.\n\nlibrary(broom)\nmod_age &lt;- lm(score ~ age, data = evals) \nmod_age |&gt; tidy()\n#&gt; # A tibble: 2 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)  4.46      0.127       35.2  1.05e-132\n#&gt; 2 age         -0.00594   0.00257     -2.31 2.13e-  2\n\nStep 2: Create a grid of predictor values.\nIn this simple example, we only have one predictor: age. So, we want to create a tibble that has a few values of age to plug into the fitted model. The seq_range() function can help with this. In the code below, we are creating a tibble called grid that contains 6 values of age: the minimum age in evals, the maximum age in evals, and four other values that are equally spaced between the minimum and the maximum. The seq_range() and data_grid() functions come from the modelr package.\n\nlibrary(modelr)\ngrid &lt;- evals |&gt;\n  data_grid(\n    age = seq_range(age, n = 6)\n  ) \ngrid\n#&gt; # A tibble: 6 × 1\n#&gt;     age\n#&gt;   &lt;dbl&gt;\n#&gt; 1  29  \n#&gt; 2  37.8\n#&gt; 3  46.6\n#&gt; 4  55.4\n#&gt; 5  64.2\n#&gt; 6  73\n\nStep 3: augment().\nNext, we want to use the augment() function from broom to use the mod_age model to predict course score for each row in grid. We will name this new tibble aug_age.\n\naug_age &lt;- augment(mod_age, newdata = grid,\n                   interval = \"confidence\")\naug_age\n#&gt; # A tibble: 6 × 4\n#&gt;     age .fitted .lower .upper\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1  29      4.29   4.18   4.40\n#&gt; 2  37.8    4.24   4.16   4.31\n#&gt; 3  46.6    4.19   4.13   4.24\n#&gt; 4  55.4    4.13   4.07   4.19\n#&gt; 5  64.2    4.08   3.99   4.17\n#&gt; 6  73      4.03   3.89   4.16\n\naugment() takes the name of the model and the name of the gridded data frame we created in the previous step. We can also obtain 95% confidence intervals for the mean response at each value of age in grid if we specify the interval = \"confidence\" argument.\nStep 4: Use ggplot2.\nThe last step is to use ggplot2 to make a meaningful plot. In this case, we can construct a plot with score on the y-axis and age on the x-axis.\n\nggplot(data = evals, aes(x = age, y = score)) +\n  geom_point() +\n  geom_line(data = aug_age, aes(x = age, y = .fitted),\n            colour = \"blue\", linewidth = 1.2)\n\n\n\n\nNote that, because the original data and the grid used for the predictions are different data frames, we have to remember to adjust the data used for each geom appropriately, taking advantage of the fact that a local data argument and local aes() aesthetics will overrid the global arguments in the ggplot() function.\nWe can then use the geom_ribbon() function to add a measure of uncertainty to our plot and generate the 95% confidence band:\n\nggplot(data = evals, aes(x = age, y = score)) +\n  geom_point() +\n  geom_line(data = aug_age, aes(x = age, y = .fitted),\n            colour = \"blue\", linewidth = 1.2) +\n  geom_ribbon(data = aug_age, aes(y = .fitted,\n                                  ymin = .lower,\n                                  ymax = .upper), \n              alpha = 0.4)\n\n\n\n\nSo, we finally get a plot of the fitted model that matches with the default plot that we get from geom_smooth(method = \"lm\")!\n\n\n\n\n\n\nImportant\n\n\n\nThis exercise is only to help us understand the steps to create the plot. For such a simple example, we would not go through the trouble of using this workflow to create this plot when it can easily be created without it.\n\n\nExercise 1. As we saw above, the grey “band” around the fitted regression line represents 95% confidence intervals for the mean response (score) for particular values of the predictor (age). In STAT 213, you also discussed 95% prediction intervals for a new observation’s response (score) for particular values of the predictor (age). What is the difference between a 95% confidence interval and a 95% prediction interval?\nExercise 2. Modify the code so that the grey band reflects 95% prediction intervals instead of 95% confidence intervals for the mean.\nExercise 3. By “hand”, verify that the .fitted value in the first row of aug_df can be calculated simply by plugging in 29 into the fitted regression equation obtained from mod_age.\nExercise 4. In data_grid(age = seq_range(age, n = 6)), why does it not matter as much what value is chosen for n in this example? Change n to be a different integer and verify that the plot does not substantially change.\nExercise 5. Fit the following model, which includes an age^2 term. Then, run the rest of the code in the chunk to obtain predictions for the age values in grid with both the mod_age model and the mod_agesq model.\n\nmod_agesq &lt;- lm(score ~ age + I(age ^ 2), data = evals) \n\ngrid &lt;- evals |&gt;\n  data_grid(\n    age = seq_range(age, n = 6)\n  ) \n\naug_agesq &lt;- augment(mod_agesq, newdata = grid,\n                     interval = \"confidence\")\naug_agesq\n#&gt; # A tibble: 6 × 4\n#&gt;     age .fitted .lower .upper\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1  29      4.29   4.12   4.47\n#&gt; 2  37.8    4.24   4.16   4.31\n#&gt; 3  46.6    4.18   4.12   4.25\n#&gt; 4  55.4    4.13   4.07   4.20\n#&gt; 5  64.2    4.08   3.97   4.20\n#&gt; 6  73      4.03   3.76   4.30\n\nUse ggplot to make a plot that has (1) the fitted line from mod_age and the fitted curve from mod_agesq, where the line/curves are coloured by the model type and (2) has the data points in the background of the plot. The code below stacks the two augmented data frames on top of each other and creates a new column called model that gives the names of the data frames as its levels.\n\nplot_df &lt;- bind_rows(lst(aug_age, aug_agesq), .id = \"model\")\nplot_df\n#&gt; # A tibble: 12 × 5\n#&gt;   model     age .fitted .lower .upper\n#&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 aug_age  29      4.29   4.18   4.40\n#&gt; 2 aug_age  37.8    4.24   4.16   4.31\n#&gt; 3 aug_age  46.6    4.19   4.13   4.24\n#&gt; 4 aug_age  55.4    4.13   4.07   4.19\n#&gt; 5 aug_age  64.2    4.08   3.99   4.17\n#&gt; 6 aug_age  73      4.03   3.89   4.16\n#&gt; # ℹ 6 more rows\n\nExercise 6. In Exercise 5, explain why the choice of n matters more for this model. Then, change n from 6 to a value that is more reasonable."
  },
  {
    "objectID": "08-model-interactions.html#visualizing-more-complex-models",
    "href": "08-model-interactions.html#visualizing-more-complex-models",
    "title": "9  Linear Model Visualization",
    "section": "\n9.2 Visualizing More Complex Models",
    "text": "9.2 Visualizing More Complex Models\nThe power of this model visualization strategy in general can really be seen in models where the coefficients are more challenging to interpret. For example, suppose that we fit the following model to the evals data:\n\nmod_comp &lt;- lm(score ~ age + bty_avg + age:bty_avg + gender,\n               data = evals)\nmod_comp |&gt; tidy()\n#&gt; # A tibble: 5 × 5\n#&gt;   term        estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 (Intercept)  5.24      0.362       14.5  2.08e-39\n#&gt; 2 age         -0.0308    0.00730     -4.22 2.91e- 5\n#&gt; 3 bty_avg     -0.204     0.0745      -2.74 6.48e- 3\n#&gt; 4 gendermale   0.213     0.0512       4.16 3.75e- 5\n#&gt; 5 age:bty_avg  0.00574   0.00156      3.69 2.53e- 4\n\nThe model contains an interaction between age and bty_avg so the coefficients involving these two terms are very tough to interpret. Our goal is to create a plot that helps interpret this model.\nWe will use the same strategy outlined in the previous section to create a data frame with predictions for various values of age, bty_avg, and gender. In data_grid(), we now need to give values not only for age, but also for bty_avg, and gender. Note that gender, the categorical predictor is a vector of its possible levels.\n\ngrid &lt;- evals |&gt;\n  data_grid(\n    age = seq_range(age, n = 6),\n    bty_avg = seq_range(bty_avg, n = 6),\n    gender = c(\"female\", \"male\")\n  ) \ngrid\n#&gt; # A tibble: 72 × 3\n#&gt;     age bty_avg gender\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; \n#&gt; 1    29    1.67 female\n#&gt; 2    29    1.67 male  \n#&gt; 3    29    2.97 female\n#&gt; 4    29    2.97 male  \n#&gt; 5    29    4.27 female\n#&gt; 6    29    4.27 male  \n#&gt; # ℹ 66 more rows\n\ndata_grid() creates one row for each age-bty_avg-gender combination. With 6 values for age, 6 values for bty_avg, and 2 values for gender, grid has 72 rows. We then gather the predictions from this grid with the mod_comp model:\n\naug_int &lt;- augment(mod_comp, newdata = grid,\n                   interval = \"confidence\")\naug_int\n#&gt; # A tibble: 72 × 6\n#&gt;     age bty_avg gender .fitted .lower .upper\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1    29    1.67 female    4.29   4.06   4.51\n#&gt; 2    29    1.67 male      4.50   4.26   4.74\n#&gt; 3    29    2.97 female    4.24   4.08   4.40\n#&gt; 4    29    2.97 male      4.45   4.28   4.62\n#&gt; 5    29    4.27 female    4.19   4.07   4.31\n#&gt; 6    29    4.27 male      4.40   4.27   4.53\n#&gt; # ℹ 66 more rows\n\nAnd the final step is to create a plot of the resulting model predictions. This is the step that requires the most critical thinking, as the plot will change depending on (1) how many models we fit (just 1 in this example) and (2) how many predictor variables we have.\nExercise 1. By hand, sketch a plot that shows the predictions from the mod_comp model in a meaningful way.\nExercise 2. Make the plot that you sketched in the previous exercise.\nExercise 3. We’ve discussed in this class the importance of showing uncertainty, when possible, using our visualizations. However, if you attempt to show uncertainty using geom_ribbon() on the plot you created, you end up with a mess. How could you modify the plot so that uncertainty is shown?\nExercise 4. Adjust one of the values for n to modify the plot in the previous exercise.\nExercise 5. Look at the help in ?seq_range and use it to adjust the trim option for age."
  },
  {
    "objectID": "08-model-interactions.html#your-turn",
    "href": "08-model-interactions.html#your-turn",
    "title": "9  Linear Model Visualization",
    "section": "\n9.3 Your Turn",
    "text": "9.3 Your Turn\nExercise 1. Fit a model of your choice with two categorical predictors, one quantitative predictor, and an interaction between the quantitative predictor and one of the categorical predictors. Construct a plot that helps interpret the coefficients from the fitted model. You do not need to show confidence bands on your plot. You should make a sketch of the plot you intend to create first!\nExercise 2. Modify the model from the previous exercise by getting rid of the interaction term. Using the workflow we have been using, construct a plot that compares the model with the interaction and the model without the interaction. Again, it might be helpful to sketch the plot first.\nExercise 3. Suppose that you want to visualize a regression model with a generic quantitative response variable \\(y\\) and 10 predictor variables \\(x_1\\), \\(x_2\\), …., \\(x_{10}\\). You are most interested in the visualizing the association between \\(x_4\\) and \\(y\\), after accounting for the effects of the other 9 predictors. Sketch an appropriate visualization for this setting. What should the values for the other 9 predictors be?"
  },
  {
    "objectID": "09-logistic.html#logistic-regression-review-class-prep",
    "href": "09-logistic.html#logistic-regression-review-class-prep",
    "title": "10  Logistic Model Visualization",
    "section": "\n10.1 Logistic Regression Review (Class Prep)",
    "text": "10.1 Logistic Regression Review (Class Prep)\nRecall that, if the response variable is categorical with only two levels, then we can fit a logistic regression model. Interpreting the coefficients of a logistic regression model, however, is more challenging than interpreting the fitted coefficients in a standard linear regression model. The interpretations rely on a statement about the multiplicative change in odds, which is generally not very intuitive.\nIn this section, we will review the overall logistic regression model, the interpretations of the fitted regression coefficients, and how to fit a logistic regression model in R.\nWe will use the titanic data set, obtained from Kaggle: https://www.kaggle.com/c/titanic/data. Each row of the data set is a passenger on the Titanic ship. The columns include:\n\n\nSurvived, a variable that is a 1 if the passenger survived the titanic shipwreck and a 0 if the passenger did not survive.\n\nPclass, either 1st, 2nd, or 3rd for the class of the passenger’s ticket\n\nSex, binary variable for sex of the passenger (male or female)\n\nAge, age of the passenger (in years).\n\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(broom)\ntheme_set(theme_minimal())\n\ntitanic_df &lt;- read_csv(here(\"data/titanic.csv\"),\n                       col_types = list(Pclass = col_factor())) |&gt;\n  mutate(Pclass = fct_recode(Pclass,\n                             \"1st\" = \"1\",\n                             \"2nd\" = \"2\",\n                             \"3rd\" = \"3\"))\ntitanic_df\n#&gt; # A tibble: 891 × 12\n#&gt;   PassengerId Survived Pclass Name       Sex     Age SibSp Parch Ticket  Fare\n#&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;  &lt;chr&gt;      &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n#&gt; 1           1        0 3rd    Braund, M… male     22     1     0 A/5 2…  7.25\n#&gt; 2           2        1 1st    Cumings, … fema…    38     1     0 PC 17… 71.3 \n#&gt; 3           3        1 3rd    Heikkinen… fema…    26     0     0 STON/…  7.92\n#&gt; 4           4        1 1st    Futrelle,… fema…    35     1     0 113803 53.1 \n#&gt; 5           5        0 3rd    Allen, Mr… male     35     0     0 373450  8.05\n#&gt; 6           6        0 3rd    Moran, Mr… male     NA     0     0 330877  8.46\n#&gt; # ℹ 885 more rows\n#&gt; # ℹ 2 more variables: Cabin &lt;chr&gt;, Embarked &lt;chr&gt;\n\n\n10.1.1 Review of Model\nWe will first consider a model with Survived as the response variable and Age as the predictor variable. The fact that Survived is a binary categorical variable means that standard linear regression would not be appropriate. Recall that, in logistic regression, we model the log-odds of survival:\n\\[\nlog\\left(\\frac{\\pi}{1 - \\pi}\\right) = \\beta_0 + \\beta_1 Age,\n\\]\nwhere \\(\\pi\\) is the probability that the passenger survived. Also, recall that the odds of survival are equal to \\(\\frac{\\pi}{(1 - \\pi)}\\) so that odds are on a scale from 0 to Infinity. We can algebraically solve for \\(\\pi\\) as:\n\\[\n\\pi = \\frac{\\text{exp}(\\beta_0 + \\beta_1 Age)}{1 + \\text{exp}(\\beta_0 + \\beta_1 Age)},\n\\]\n\n10.1.2 Fitting the Model\nWe can fit the model in R with the glm() function, which is very similar to lm(), except that there is an extra argument family to specify that we want to use the \"binomial\" to fit a logistic regression model.\n\ntitanic_mod &lt;- glm(Survived ~ Age,\n                   data = titanic_df, family = \"binomial\")\ntitanic_mod\n#&gt; \n#&gt; Call:  glm(formula = Survived ~ Age, family = \"binomial\", data = titanic_df)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)          Age  \n#&gt;    -0.05672     -0.01096  \n#&gt; \n#&gt; Degrees of Freedom: 713 Total (i.e. Null);  712 Residual\n#&gt;   (177 observations deleted due to missingness)\n#&gt; Null Deviance:       964.5 \n#&gt; Residual Deviance: 960.2     AIC: 964.2\n\nThe broom package functions augment(), tidy(), and glance() can also be used on models fit with glm():\n\ntitanic_mod |&gt; tidy()\n#&gt; # A tibble: 2 × 5\n#&gt;   term        estimate std.error statistic p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 (Intercept)  -0.0567   0.174      -0.327  0.744 \n#&gt; 2 Age          -0.0110   0.00533    -2.06   0.0397\n\nWe can also make a plot of the fitted model by using geom_smooth() with method = \"glm\":\n\nggplot(data = titanic_df, aes(x = Age, y = Survived)) +\n  geom_jitter(height = 0.05) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWe get a warning that 177 rows have been removed because we do not have Age data for 177 of the passengers.\n\n\nWe also see that the model predicts that older passengers tended to have a lower survival probability.\n\n10.1.3 Interpreting Coefficients\nWe next interpret the coefficients from this simple logistic regression model. Because our interpretations involve explanations about odds, these coefficient interpretations are not particularly useful, especially to the general public who use the words “probability” and “odds” somewhat interchangeably (for statisticians, these are different things though!).\n\ntitanic_mod |&gt; tidy()\n#&gt; # A tibble: 2 × 5\n#&gt;   term        estimate std.error statistic p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 (Intercept)  -0.0567   0.174      -0.327  0.744 \n#&gt; 2 Age          -0.0110   0.00533    -2.06   0.0397\n\nBecause this model was linear on the log-odds scale, the interpretation of the -0.00261 value is linear on the log-odds scale: The model predicts that a one year increase in age is associated with a 0.00261 decrease in the log-odds of survival.\nThis interpretation is practically useless: what does it mean to have that amount decrease in log-odds of survival? If we take \\(e^{-0.00261} = 0.997\\), we can interpret the resulting value on the odds scale as a multiplicative change in odds. You likely discussed why you can do this in STAT 213 but we won’t focus on it again here. The resulting interpretation is something like “The model predicts that a one year increase in age is associated with a multiplicative change in the odds of survival of 0.997 times.” In other words, the model predicts that, for a one year increase in age, odds of survival are the predicted odds of survival of the previous year of age times 0.997.\nAgain, this interpretation is not particularly useful or effective. And, we are in the simplest possible case of logistic regression: a model with just one predictor! Interpretations are even more challenging when there are multiple predictors, interactions, squared terms, etc. The goal of the next section is to use visualization to help communicate model results from logistic regression.\nExercise 1. Fit a logistic regression model with Fare as a predictor. Obtain summary output with tidy() and use ggplot2 to construct a plot of the model.\nExercise 2. Fit a logistic regression model with Sex as the predictor. Make an attempt at interpreting the resulting fitted coefficient for Sex on either the log-odds scale or the odds scale (keeping in mind that Sex is categorical so your interpretation should be comparing odds of survival of female and male passengers)."
  },
  {
    "objectID": "09-logistic.html#visualizing-logistic-regression-models",
    "href": "09-logistic.html#visualizing-logistic-regression-models",
    "title": "10  Logistic Model Visualization",
    "section": "\n10.2 Visualizing Logistic Regression Models",
    "text": "10.2 Visualizing Logistic Regression Models\nWarm Up Exercise. Jittering allowed us to see the data points on the logistic regression plot of Survival vs. Age earlier, but, another option, is to generate a rug plot with geom_rug(). Examine the link https://ggplot2.tidyverse.org/reference/geom_rug.html and modify the plot to have the “rug” tick marks on the top and bottom of the graph instead of the points.\n\n\nTo help communicate what a fitted logistic regression model means, in terms of predicted probability (as opposed to predicted odds or log-odds), we will again make use of the modelr package. We will use the same overall strategy as the previous section on using modelr to visualize models from standard linear regression models.\nOur basic strategy for visualizing models is to\n\nfit the model with glm().\nconstruct a grid of predictor values with the data_grid() function from the modelr package.\nUse the augment() function from the broom package on the data grid in (2) to to obtain predicted probabilities according to the model for each row in the grid.\nUse ggplot2 to construct a meaningful plot with the predicted probabilities.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe only difference here compared to our strategy for standard linear models is that we want to make a plot of the predicted probability in logistic regression.\n\n\nSuppose we fit a logistic regression model with Survived as the response variable and Age, Sex, and Pclass as predictor variables:\n\ntitanic_large &lt;- glm(Survived ~ Age + Sex + Pclass, data = titanic_df,\n                     family = \"binomial\")\ntitanic_large |&gt; tidy()\n#&gt; # A tibble: 5 × 5\n#&gt;   term        estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 (Intercept)   1.20     0.253        4.74 2.19e- 6\n#&gt; 2 Age          -0.0370   0.00766     -4.83 1.36e- 6\n#&gt; 3 Sexmale      -2.52     0.207      -12.2  4.81e-34\n#&gt; 4 Pclass1st     2.58     0.281        9.17 4.76e-20\n#&gt; 5 Pclass2nd     1.27     0.244        5.21 1.92e- 7\n\n\n\n\n\n\n\nNote\n\n\n\nLet’s take a moment to appreciate that interpreting these coefficients in terms of a multiplicative change in odds of survival would not be that informative.\n\n\nWe next use modelr to create a grid of predictors that we want to make predictions for and then gather these predictions using the titanic_large model:\n\nlibrary(modelr)\ngrid &lt;- titanic_df |&gt;\n  data_grid(\n    Age = seq_range(Age, n = 10),\n    Sex = c(\"female\", \"male\"),\n    Pclass = c(\"1st\", \"2nd\", \"3rd\")\n  ) \ngrid\n#&gt; # A tibble: 60 × 3\n#&gt;     Age Sex    Pclass\n#&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n#&gt; 1  0.42 female 1st   \n#&gt; 2  0.42 female 2nd   \n#&gt; 3  0.42 female 3rd   \n#&gt; 4  0.42 male   1st   \n#&gt; 5  0.42 male   2nd   \n#&gt; 6  0.42 male   3rd   \n#&gt; # ℹ 54 more rows\n\n\naug_surv &lt;- augment(titanic_large, newdata = grid,\n                    se_fit = TRUE)\naug_surv\n#&gt; # A tibble: 60 × 5\n#&gt;     Age Sex    Pclass .fitted .se.fit\n#&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1  0.42 female 1st     3.76     0.399\n#&gt; 2  0.42 female 2nd     2.45     0.317\n#&gt; 3  0.42 female 3rd     1.18     0.250\n#&gt; 4  0.42 male   1st     1.24     0.357\n#&gt; 5  0.42 male   2nd    -0.0711   0.296\n#&gt; 6  0.42 male   3rd    -1.34     0.246\n#&gt; # ℹ 54 more rows\n\nWe will complete the rest of this exercise as a class.\nExercise 1. Examine the .fitted column in aug_surv. What are these values? Why are they not between 0 and 1?\nExercise 2. Make a plot of the values of .fitted (without modifying them).\nExercise 3. Convert .fitted to predicted probabilities \\(\\hat{\\pi}\\) with the following formula. Note that exp(3.2) is R code for \\(e^{3.2}\\)\n\\[\n\\hat{\\pi} = \\frac{e^{pred}}{(1 + e^{pred})}\n\\]\nExercise 4: Make the plot again, using predicted probabilities instead of .fitted values."
  },
  {
    "objectID": "09-logistic.html#your-turn",
    "href": "09-logistic.html#your-turn",
    "title": "10  Logistic Model Visualization",
    "section": "\n10.3 Your Turn",
    "text": "10.3 Your Turn\nExercise 1. Add an Sex:Pclass interaction term to the previous model and fit the model with glm().\nExercise 2. Obtain model predictions for both the previous model (without the interaction) and the new model (with the interaction). Convert the resulting .fitted column to predicted probabilities.\nExercise 3. Construct a graphic that shows how the two models differ.\nExercise 4. How would you incorporate uncertainty into your graph?"
  },
  {
    "objectID": "14-other-topics.html#outliers-and-area-plots-class-prep",
    "href": "14-other-topics.html#outliers-and-area-plots-class-prep",
    "title": "11  Other Topics and Models",
    "section": "\n11.1 Outliers and Area Plots (Class Prep)",
    "text": "11.1 Outliers and Area Plots (Class Prep)\n\n11.1.1 Outliers\nVisualizations when there are large outliers present can be challenging. On the one hand, removing an outlier from a visualization for no reason besides “it’s an outlier” eliminates an otherwise perfectly valid data point. On the other hand, including the outlier can make the visualization essentially meaningless for all of the other points.\nLet’s look at this issue with an example. You may have worked with the mammals data set (or something similar) in STAT 213 to investigate statistical transformations for modeling. A scatterplot of mammal brain weight vs. mammal body weight is given below. The outliers here are two species of elephants included in the data set.\n\nlibrary(MASS)\nlibrary(tidyverse)\ntheme_set(theme_minimal())\n\nggplot(data = mammals, aes(x = body, y = brain)) +\n  geom_point()\n\n\n\n\nWe can see the issue: the visualization tells us that there are two large outliers, but that’s about it. We can’t distinguish the values for most of the other mammals in the data set.\nThere are a few strategies to deal with such outliers. First, you could remove them and state in a figure caption that you removed them.\n\nggplot(data = mammals |&gt; filter(body &lt;= 600), aes(x = body, y = brain)) +\n  geom_point() +\n  labs(caption = \"Two species of elephant in the data were removed before constructing this plot.\",\n       title = \"Option 1\")\n\n\n\n\nSecond, you could use use axis breaks:\n\n## install.packages(\"ggbreak\")\nlibrary(ggbreak)\nggplot(data = mammals, aes(x = body, y = brain)) +\n  geom_point() +\n  scale_x_break(c(750, 2500, 2700, 6400)) +\n  labs(title = \"Option 2\")\n\n\n\n\nYou could do some things to make this plot look a little nicer, but I personally do not like scale breaks. I think they make the plot look confusing and potentially misleading as the reader has to pay really close attention to the x-axis to interpret the plot correctly.\nA third option is to do exactly what you would have done in STAT 213: perform a statistical transformation of the data. This has exactly the same benefits and drawbacks as it did in modeling: your model/plot works a lot better at the cost of interpretation. The following uses the scale_x_log10() and scale_y_log10() to plot a log-log transformation of brain vs. body weight.\n\nlibrary(MASS)\nggplot(data = mammals, aes(x = body, y = brain)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_y_log10() +\n  labs(title = \"Option 3\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe scales on the x and y-axis now increase multiplicatively. So, they are in general a bit harder for any reader to interpret.\n\n\nWe can also figure out how to display a natural log transformation (like you probably would have done in STAT 213), but that is a fair bit of work. When making this graph, this stack overflow post helped me out: https://stackoverflow.com/questions/43702945/making-an-axis-give-natural-log-instead-of-log10-when-log-y-in-r-plot\n\nlibrary(scales) \nggplot(data = mammals, aes(x = body, y = brain)) +\n  geom_point() +\n    scale_y_continuous(trans = log_trans(), \n                         breaks = trans_breaks(\"log\", function(x) exp(x)),\n                         labels = trans_format(\"log\", math_format(e ^ .x))) +\n    scale_x_continuous(trans = log_trans(), \n                         breaks = trans_breaks(\"log\", function(x) exp(x)),\n                         labels = trans_format(\"log\", math_format(e ^ .x)))\n\n\n\n\n\n11.1.2 Line Plots and Area Plots\nLine plots are useful when you are interested in exploring the relationship between two quantitative variables and the quantitative variable on the x-axis only has one y value for each x value (or, if also graphing a categorical variable, one y value for each x value in for each level of the categorical variable). One instance where this commonly arises is for an x-axis variable of some measure of time.\nFor example, in the babynames data set from the babynames R package, there are variables year, sex, name, and n (number of babies). In each row, the n value is the number of births of babies with that particular name of that sex in that year.\n\nlibrary(babynames)\nbabynames\n#&gt; # A tibble: 1,924,665 × 5\n#&gt;    year sex   name          n   prop\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;\n#&gt; 1  1880 F     Mary       7065 0.0724\n#&gt; 2  1880 F     Anna       2604 0.0267\n#&gt; 3  1880 F     Emma       2003 0.0205\n#&gt; 4  1880 F     Elizabeth  1939 0.0199\n#&gt; 5  1880 F     Minnie     1746 0.0179\n#&gt; 6  1880 F     Margaret   1578 0.0162\n#&gt; # ℹ 1,924,659 more rows\n\nIf we want to plot the number of births (n) for a particular name, we might consider a line plot instead of a point plot:\n\nbaby_matt &lt;- babynames |&gt; filter(name == \"Matthew\" & sex == \"M\")\nggplot(data = baby_matt, aes(x = year, y = n)) +\n  geom_line()\n\n\n\n\nThe baby_matt data set has one y value (one n value) for each x value (year), so a line plot makes sense. We can also construct a line plot with different colours for different names. Again, for each level of name, there is only one y-value for each x-value so a line plot makes sense.\n\nbaby_friends &lt;- babynames |&gt; filter(name == \"Monica\" & sex == \"F\" |\n                                    name == \"Phoebe\" & sex == \"F\" |\n                                    name == \"Rachel\" & sex == \"F\" |\n                                    name == \"Ross\" & sex == \"M\" |\n                                    name == \"Joseph\" & sex == \"M\" |\n                                    name == \"Chandler\" & sex == \"M\")\nggplot(data = baby_friends, aes(x = year, y = n)) +\n  geom_line(aes(colour = name)) +\n  scale_colour_brewer(palette = \"Accent\")\n\n\n\n\n\nArea plots are an alternative to line plots, most often used for time series data. Like line plots, they are useful when you have a single \\(y\\) per unit of \\(x\\) (as is often the case with time) and when you want to examine the trend of \\(y\\) through \\(x\\) (again \\(x\\) is often time).\n\n\n\n\n\n\nImportant\n\n\n\n\n\nArea plots should only be used when there is a meaningful relationship to 0 for the response, \\(y\\), because the area between \\(y\\) and \\(0\\) will be shaded.\n\n\n\nBecause of this shading, area charts can be used to emphasize absolute gains and losses in the response. One common place where they would get used is to display a stock price through time.\nThe following code plots the now infamous GameStop stock through time by scraping stock data with the quantmod package:\n\nlibrary(quantmod)\nlibrary(lubridate)\n\nstart &lt;- ymd(\"2011-01-01\")\nend &lt;- ymd(\"2022-3-17\")\ngetSymbols(c(\"GME\", \"SPY\"), src = \"yahoo\",\n           from = start, to = end)\n#&gt; [1] \"GME\" \"SPY\"\n\ndate_tib &lt;- as_tibble(index(GME)) |&gt;\n  rename(start_date = value)\ngme_tib &lt;- as_tibble(GME)\nspy_tib &lt;- as_tibble(SPY)\nall_stocks &lt;- bind_cols(date_tib, gme_tib, spy_tib)\n\nstocks_long &lt;- all_stocks |&gt;\n  dplyr::select(start_date, GME.Adjusted, SPY.Adjusted) |&gt;\n  pivot_longer(2:3, names_to = \"Stock_Type\", values_to = \"Price\") |&gt;\n  mutate(Stock_Type = fct_recode(Stock_Type,\n                                 GameStop = \"GME.Adjusted\",\n                                 `S & P 500` = \"SPY.Adjusted\"\n                                 ))\n\nCompare the line and area charts for the Gamestop stock. Note the common trick to use the same colour and fill for geom_line() and geom_area(), but to decrease the transparency for geom_area().\n\nstocks_gme &lt;- stocks_long |&gt; filter(Stock_Type == \"GameStop\")\nggplot(data = stocks_gme, aes(x = start_date, y = Price)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\nggplot(data = stocks_gme, aes(x = start_date, y = Price)) +\n  geom_line(colour = \"blueviolet\") +\n  geom_area(fill = \"blueviolet\", alpha = 0.3) +\n  theme_minimal()\n\n\n\n\nAnd for the S and P 500:\n\nstocks_spy &lt;- stocks_long |&gt; filter(Stock_Type == \"S & P 500\")\nggplot(data = stocks_spy, aes(x = start_date, y = Price)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\nggplot(data = stocks_spy, aes(x = start_date, y = Price)) +\n  geom_line(colour = \"green4\") +\n  geom_area(fill = \"green4\", alpha = 0.3) +\n  theme_minimal()\n\n\n\n\nExercise 1. For the three options for graphing outliers (removal, axis breaks, transforming the data), which do you think is best for the mammal data set? Give a one sentence reason.\nExercise 2. Choose your own names to visualize in the babynames data set. Construct the line plot.\nExercise 3. In your filter() step for your babynames visualization, remove the sex == \"F\" or sex == \"M\" for one of the names. Why is a line plot no longer a good way to visualize the new data set.\nExercise 4. Construct an area plot that shows both the Gamestop stock price and the S and P 500 stock price, using different colours and fills for the two stocks. Why might a line plot be a better visualization for this example?"
  },
  {
    "objectID": "14-other-topics.html#visualizing-large-data-sets",
    "href": "14-other-topics.html#visualizing-large-data-sets",
    "title": "11  Other Topics and Models",
    "section": "\n11.2 Visualizing “Large” Data Sets",
    "text": "11.2 Visualizing “Large” Data Sets\nA data set could be “large” if it has a very large number of observations (large n), if it has a very large number of variables (large p), or both. Let’s start by looking at a data set with a somewhat large number of observations, the diamonds data set in the ggplot2 package. The diamonds data set contains variables on characteristics of diamonds, like the diamond’s price, cut, clarity, carat, etc.\n\n11.2.1 Large n\nLet’s first check how many observations are actually in the data set:\n\nlibrary(tidyverse)\ndiamonds |&gt; nrow()\n#&gt; [1] 53940\n\nSo, there are 53940 diamonds in the data set. If we try to make a basic scatterplot of price vs. carat, it doesn’t look so great, and, it takes a few seconds for r to actually plot all of the individual points.\n\nggplot(data = diamonds, aes(x = carat, y = price)) +\n  geom_point()\n\n\n\n\nWe’ve talked about controlling the point transparency (alpha) as one method to make the plot look a little more aesthetically pleasing but there are other options. Another solution is to use geom_hex() to get a cleaner look at the data:\n\nggplot(data = diamonds, aes(x = carat, y = price)) +\n  geom_hex() +\n  scale_fill_viridis_c() +\n  theme_minimal()\n\n\n\n\ngeom_hex() automatically maps the number of points in each hexbin to a count fill aesthetic. We can more easily see from this plot that the large majority of diamonds in the data set are smaller diamonds with a lower price, while still seeing the general increasing price - carat trend.\n\n11.2.2 Really Large n\nBut what if your sample size was really large. Suppose n was 10,000,000. Using a geom that maps one point to each row will cause R to crash. Other geom’s, like geom_hex() can still work okay after a few seconds.\n\nlibrary(tidyverse)\nn &lt;- 10000000\nx &lt;- rnorm(n, 0, 1)\ny &lt;- x * rgamma(n, 3, 2)\ndf &lt;- tibble(x = x, y = y)\n\n## if you run this next line, R will probably crash!\n## ggplot(data = df, aes(x = x, y = y)) +\n##  geom_point()\n\n# ggplot(data = df, aes(x = x, y = y)) +\n#   geom_hex() +\n#   scale_fill_viridis_c()\n\nAnother option though, is to sample your data before plotting. Do you really need all 10,000,000 observations to see the patterns in your data? Probably not (though we will discuss this more in an exercise). So, you can use the slice_sample() function to choose some observations to plot, especially in the initial stages of your exploration:\n\ndf_small &lt;- df |&gt; slice_sample(n = 10000)\nggplot(data = df_small, aes(x = x, y = y)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth()\n\n\n\n\n\n11.2.3 Large p\nAnother way in which a data set can be “large” is if there are a large number of variables that you’re interested in. Let’s look at some batting statistics from a data set on Major League Baseball players in the openintro R package.\n\n## install.packages(\"openintro\")\nlibrary(openintro)\nmlb_small &lt;- mlb_players_18 |&gt; filter(games &gt; 50 & position != \"P\") ## only look at players \n## who play at least 50 games in the season and are not pitchers\n\nThere are a lot of statistics here pertaining to batting: AB, R, H, doubles, triples, HR, RBI, walks, strike_outs, stolen_bases, caught_stealing_base, AVG, OBP, SLG, and OPS. We can’t reasonably make a scatterplot of each pair of variables, but we can do have some other options. One that we have used in STAT 213 and STAT 234 is the ggpairs() function from the GGally package. However, ggpairs() cannot take that many variables, so we will only use 7 variables in this example:\n\nlibrary(GGally)\nggpairs(data = mlb_small, columns = 6:11)\n\n\n\n\nIf we wanted to quickly look at the correlations between each pair of variables, the GGally package has a ggcorr() function:\n\nggcorr(data = mlb_small |&gt; dplyr::select(4:ncol(mlb_small)))\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThere is a bit of danger here: a correlation coefficient assumes a linear relationship so, if the relationship between two variables is non-linear, it would not be properly captured by such a plot.\n\n\nWe are also not able to see outliers from this plot, so, while it is a useful starting point, it should not be an ending point. Correlation plots and pairs plots are both useful for exploratory analysis before creating a model or before using a statistical learning method. They can also be used to help detect multicollinearity among potential predictors in the data set.\nExercise 1. In the example where we sampled from the very large data set to construct a graphic, can you think of some possible flaws in this approach?"
  },
  {
    "objectID": "14-other-topics.html#statistical-learning-models",
    "href": "14-other-topics.html#statistical-learning-models",
    "title": "11  Other Topics and Models",
    "section": "\n11.3 Statistical Learning Models",
    "text": "11.3 Statistical Learning Models\nSome of you have enrolled in, are enrolled in, or will enroll in CS/DATA/STAT 352: Statistical and Machine Learning. We can use visualization to better communicate the results of each of the statistical learning methods you learn about, including k-nearest-neighbors, clustering algorithms, linear discriminant analysis, principal components analysis, decision trees, random forests, lasso regression, etc.\nHowever, because the course is neither a pre-requisite nor a co-requisite for this course, we will just give a short example. If you decide to use a statistical learning method for your final project, you should be able to use a search engine to help you see how others have visualized that particular method. Usually, someone will have written a package that can be used to help with the visualization.\nFor a short example here, a quick google search on how to visualize the decision boundaries in a decision tree leads to https://www.r-bloggers.com/2020/03/visualizing-decision-tree-partition-and-decision-boundaries/. The parttree package, written by Grant McDermott can be used with the rpart package to construct:\n\nlibrary(tidymodels)\nlibrary(titanic)\n\n## remotes::install_github(\"grantmcdermott/parttree\")\nlibrary(parttree)\nset.seed(123) ## For consistent jitter\n\ntitanic_train$Survived = as.factor(titanic_train$Survived)\n\n## Build our tree using parsnip (but with rpart as the model engine)\nti_tree =\n  decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\") %&gt;%\n  fit(Survived ~ Pclass + Age, data = titanic_train)\n\n## Plot the data and model partitions\ntitanic_train %&gt;%\n  ggplot(aes(x = Pclass, y = Age)) +\n  geom_jitter(aes(colour = Survived), alpha = 0.7) +\n  geom_parttree(data = ti_tree, aes(fill = Survived), alpha = 0.1) +\n  theme_minimal()\n\n\n\n\nNote that I’ve copied the code directly from the R bloggers site for this example. Additionally, the code used to create the ti_tree object is from the tidymodels ecosystem of statistical learning. If you decide to do something involving statistical learning, I would highly encourage you to learn how to fit your model with tidymodels https://www.tidymodels.org/start/models/."
  },
  {
    "objectID": "14-other-topics.html#your-turn",
    "href": "14-other-topics.html#your-turn",
    "title": "11  Other Topics and Models",
    "section": "\n11.4 Your Turn",
    "text": "11.4 Your Turn\nThe purpose of the following exercises is to emphasize that the approach of only looking at a correlation plot is flawed. The correlation plot might be a useful starting point for exploration, but it should not be the end point.\nExercise 1. Examine the following correlation plot from simulated data. Based on the correlation plot alone, does it look like there is any relationship between x1 and y? Does it look like there is any relationship between x2 and y?\n\nset.seed(1414111)\nx1 &lt;- rnorm(100, 0, 1)\nx2 &lt;- seq(-1, 1, length.out = 100)\ny &lt;- 4 + 0 * x1 + 1 * x2^2 + rnorm(100, 0, 0.1)\ndf &lt;- tibble(x1 = x1, x2 = x2, y = y)\nggcorr(data = df)\n\n\n\n\nExercise 2. Construct a simple scatterplot of y vs. x1. Then, construct a simple scatterplot of y vs. x2. Does it look like there is a relationship between y and x1? Between y and x2?\nExercise 3. Using what you saw from the previous two exercises, explain why it is flawed to only look at a correlation plot to assess the relationship between variables."
  },
  {
    "objectID": "10-ethics.html#visualization-ethics-class-prep",
    "href": "10-ethics.html#visualization-ethics-class-prep",
    "title": "12  Ethics",
    "section": "\n12.1 Visualization Ethics (Class Prep)",
    "text": "12.1 Visualization Ethics (Class Prep)\nLike in data science in general, we need to consider ethical implications of any data visualization. For some data sets, like a data set on Pokemon statistics, the consequences of misuse of the data or a poor visualization would be minor. However, for other data sets, like data on police killings in the United States or data on school shootings, misuse of the data or a poor visualization could have catastrophic consequences.\nIn this section, we will consider general ethics principles that can be applied to almost any data visualization. We will then examine sensitive data on historical lynchings in the United States to discuss some principles of conscientious data visualization.\n\n12.1.1 General Principles\nRead Sections 8.1 through 8.3 of Modern Data Science with R, found here.\nExercise 1. Pick 1 of the 3 examples (stand your ground, climate change, or covid cases), and summarize why that graphic is not ethical. Make sure to include the context that the graph was created in from Section 8.3 in your explanation.\nRead the short Section 8.5 of Modern Data Science with R, found here, stopping at Section 8.5.1.\nExercise 2. Choose 2 of the 12 principles of ethical practice of data science and come up a (possibly hypothetical) scenario where each of your two chosen principles would come into play.\n\n\n12.1.2 Sensitive Data Visualizations\n\n\n\n\n\n\nWarning\n\n\n\nThe following section deals with sensitive data on lynchings in the United States. If, at any point, working through these examples is detrimental to your mental health, please stop immediately. You may request an alternative assignment with no explanation necessary.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen visualizing sensitive data, we not only need to consider the context that the data came from, but also we need to critically examine the ramifications of producing the graphic.\n\n\nIn particular, if our data points are human beings that were killed, tortured, or mistreated, there is danger in “dehumanizing” these people to single data points.\nConsider the following two examples, both of which map lynchings in the United States.\nExample 1: First,  this choropleth map shows reported lynchings in the southern United States between 1877 and 1950. Take a few moments to zoom in and out of the map to get a sense about how the data is presented.\nExample 2: Second,  this map shows similar data but the data is presented as individual points instead of on a chloropleth map. Take a few moments to zoom in and out of the map to get a sense about how the data is presented.\nExercise 3. Which of the two visualizations better humanizes the victims of lynching? Why?\nExercise 4. Suppose that you want to humanize the victims more in a static, non-interactive graph. Brainstorm ways that you could achieve this.\nExercise 5. A deeper discussion of these sites appears in  this paper. Read the Introduction section on TayTweets and write 3 major takeaways from this section.\nExercise 6. The authors of the book  Data Feminism argue that “leveraging emotion” might help us better communicate with data. Read the first four paragraphs of Chapter 3 of the  Data Feminism book. One example the authors use pertains to deaths from gun violence in the United States. Examine  this animated chart. Explain how the chart uses emotion to convey the losses due to gun violence better than, for example, a statement that 11,356 people were killed from gun violence in 2018."
  },
  {
    "objectID": "10-ethics.html#challenger-case-study",
    "href": "10-ethics.html#challenger-case-study",
    "title": "12  Ethics",
    "section": "\n12.2 Challenger Case Study",
    "text": "12.2 Challenger Case Study\nIn this section, we will look at an example from the Challenger space shuttle launch. One of the more important things to take away from this section is that, if you choose to exclude data in a data visualization, then\n\nYou should have a good reason and\nYou should explicitly state that you excluded certain data points and state your reason for doing so.\n\nA description of the problem follows:\nOn January 28, 1986, the NASA space shuttle program launched its 25th shuttle flight from Kennedy Space Center in Florida. Seventy-three seconds into the flight, the external fuel tank collapsed and spilled liquid oxygen and hydrogen. These chemicals ignited, destroying the space shuttle Challenger and killing all seven crew members (including Ronald E. McNair, for whom the McNair program is named, and Christa McAuliffe, a school teacher).\n\n\n\n\n\n\nWarning\n\n\n\nWatching the Youtube video linked below is completely optional. This was live coverage of the failed launch that can be difficult to watch, linked here.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is also a Netflix documentary special called Challenger: The Final Flight that covers some of the following issues.\n\n\nInvestigations showed that an O-ring seal in the right solid rocket booster failed to isolate the fuel supply. Because of their size, the rocket boosters were built and shipped in separate sections. A forward, center, and aft field joint connected the sections. Two O-rings (one primary and one secondary), which resemble giant rubber bands 0.28 inches thick, but 37 feet in diameter, were used to seal the field joints between each of the sections.\nThe O-ring seals were intended to stop the gases inside the solid rocket booster from escaping. However, the cold outside air temperature caused the O-rings to become brittle and fail to seal properly. Gases at 5800 degrees F escaped and burned a hole through the side of the rocket booster. Could this disaster have been avoided? The problem with the O-rings was recognized by the engineers who designed them. One day before the flight, the predicted temperature for the launch was 26 degrees to 29 degrees F. The actual launch temperature was 31 degrees F. Concerned that the O-rings would not seal at these temperatures, the engineers opposed the launching of the Challenger and tried to convince officials not to launch the shuttle. Even though they understood the severity of the problem, the engineers were unable to properly communicate their evidence to officials.\nA very poor data visualization contributed to the miscommunication. The graph that was used to make the decision to launch excluded data from launches without any incidents. You can examine the graph used on page 5 of this link. The y-axis is O-ring damage while the x-axis is temperature.\nHowever, when all of the data points are used, from both flights with an incident and without incidents, a trend becomes much more clear.\n\nExercise 1. Read in the Challenger.csv data file, which contains the variables Temperature (the temperature of the launch in Farenheit) and SuccessfulLaunch (a 1 if the launch had no O-ring damage and a 0 if the launch was did have some O-ring damage). Note that, even with some O-ring damage, a launch would not necessarily fail. Each row corresponds to a test launch of the shuttle.\n\nlibrary(tidyverse)\nlibrary(here)\ntheme_set(theme_minimal())\nchallenger_df &lt;- read_csv(here(\"data/Challenger.csv\"))\n\nExercise 2. Fit a logistic regression model with SuccessfulLaunch as the response variable and Temperature as the predictor. Interpret the resulting coefficient for Temperature in terms of either the log-odds of a successful launch or in terms of the odds of a successful launch.\nExercise 3. Construct a visualization of the fitted regression model on the probability scale (so not on the odds or log-odds scale).\nExercise 4: To your plot, add a vertical line at 31 degrees Farenheit, which was the temperature at the time of the actual launch.\nExercise 5. Write a memo urging NASA to consider postponing the flight. Consider what would be more helpful in your memo: the regression coefficient interpretation or the visualization."
  },
  {
    "objectID": "10-ethics.html#your-turn",
    "href": "10-ethics.html#your-turn",
    "title": "12  Ethics",
    "section": "\n12.3 Your Turn",
    "text": "12.3 Your Turn\nExercise 1. There were analysis mistakes that likely contributed to the Challenger tragedy, but, would you consider the negligence of including data points unethical. Give a reason for your choice.\nExercise 2. Suppose that you complete an analysis but you make a mistake in the process. As a result, there is some harm to a person or a group of people. Would you consider this an ethical violation? What else might you want to know about this situation before making a decision on whether your mistake was a violation of ethics?"
  },
  {
    "objectID": "11-when-to-use.html#leaflet-class-prep",
    "href": "11-when-to-use.html#leaflet-class-prep",
    "title": "13  Introduction to Interactivity",
    "section": "\n13.1 Leaflet (Class Prep)",
    "text": "13.1 Leaflet (Class Prep)\nLeaflet is a useful tool to make interactive maps. It is powered by JavaScript but the R package leaflet lets us take advantage of many of its features. The purpose of this section is to show what kinds of interactive maps we can make with leaflet, but we will not learn all of the details or the customization options of the leaflet package.\nThe following code is slightly modified from a blog called Linear Fragility. The goal is to make a map of breweries across the United States that appear in https://thebeermonthclub.com/. The data comes from Kaggle and contains information on 2497 breweries across the United States.\nBefore we proceed to making the map, we note that, in the blog post, the writer mentions that he uses an R package on his GitHub site.\n\n\n\n\n\n\nImportant\n\n\n\nNot all R packages are submitted to CRAN and are readily installable. But anyone can write their own R package and not submit it to CRAN, which is where packages are stored that can be installed with install.packages().\n\n\nA popular place to store such a package is a GitHub site: https://github.com/li-wen-li/uszipcodes. Packages on GitHub are not checked by anyone, so they may or may not actually work, but many are still reliable.\nTo install a package from GitHub, we can use the devtools package. The name_of_package::name_of_function() is syntax used when we want to denote the package that a particular function comes from. For example, we might use dplyr::filter() to let the reader of our code know that the filter() function is in the dplyr package.\n\n## install.packages(\"devtools\")\nlibrary(devtools)\n## devtools::install_github(\"li-wen-li/uszipcodes\")\nlibrary(uszipcodes)\n\nNext, we can read in the beer data set:\n\nlibrary(tidyverse)\nlibrary(here)\ntheme_set(theme_minimal())\n\nbeers &lt;- read_csv(here(\"data/breweries.csv\"))\n\nVariables include the brewery_name, type, address, and state. But, you’ll notice that the beers data set does not include any spatial coordinates that give the locations of the breweries. There is, however, a variable called address: we will use the uszipcodes package to extract the zip code from the address variable so that we can map a brewery to its zip code.\n\nraw_zip &lt;- uszipcodes::get_zip(beers$address)\nbeers$Zip &lt;- as.integer(uszipcodes::clean_zip(raw_zip))\n\nWhen you run the code to add the variable Zip to the beers data set, you’ll notice that you’ll get a warning that there are some NA values that were introduced. The functions are not perfect: there are some zip codes that were not properly extracted from the address.\nFinally, we join the zip codes, along with their latitudes and longitudes, to the beers data:\n\n## only keep zip, lat, and long\nzip_tab &lt;- zip_table |&gt; dplyr::select(Zip, Latitude, Longitude)\nbeer_location &lt;- inner_join(beers, zip_tab)\nbeer_location\n#&gt; # A tibble: 2,327 × 9\n#&gt;   brewery_name     type  address website state state_breweries   Zip Latitude\n#&gt;   &lt;chr&gt;            &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n#&gt; 1 Valley Brewing … Brew… PO Box… http:/… cali…             284 95204     38.0\n#&gt; 2 Valley Brewing … Brew… 157 Ad… http:/… cali…             284 95204     38.0\n#&gt; 3 Valley Brewing … Micr… 1950 W… http:/… cali…             284 95203     38.0\n#&gt; 4 Ukiah Brewing C… Brew… 102 S.… http:/… cali…             284 95482     39.2\n#&gt; 5 Tustin Brewing … Brew… 13011 … http:/… cali…             284 92780     33.7\n#&gt; 6 Trumer Brauerei  Micr… 1404 4… http:/… cali…             284 94608     37.8\n#&gt; # ℹ 2,321 more rows\n#&gt; # ℹ 1 more variable: Longitude &lt;dbl&gt;\n\nThe next part of the code makes the content data set, which adds a variable called popup that contains the brewery website and name. This variable eventually contains the values the “pop up” on the map when we hover over a data point.\n\ncontent &lt;- beer_location |&gt;\n  mutate(popup = paste0('&lt;a href =', beer_location$website, '&gt;',\n                        beer_location$brewery_name, '&lt;/a&gt;'))\n\nThe remaining code comes from the leaflet package. As usual, it is helpful to run the code “pipe by pipe” to see what each piece is doing.\n\nlibrary(leaflet)\n\nbeer_map &lt;- leaflet(beer_location) |&gt;\n  setView(lng = -98.583, lat = 39.833, zoom = 4) |&gt;\n  addTiles() |&gt;\n  addProviderTiles(providers$Wikimedia) |&gt;\n  addMarkers(lng = beer_location$Longitude, lat = beer_location$Latitude,\n             clusterOptions = markerClusterOptions(),\n             popup = content$popup)\n\n\nbeer_map\n\n\n\n\n\n\nExercise 1. Why is inner_join() the most appropriate join function to use here in this example? What observations will an inner_join() get rid of from beers? from zip_tab?\nExercise 2a. Run leaflet(beer_location) |&gt; setView(lng = -98.583, lat = 39.833, zoom = 4) and explain what the setView() function does.\nExercise 2b. Run leaflet(beer_location) |&gt; setView(lng = -98.583, lat = 39.833, zoom = 4) |&gt; addTiles() and explain what the addTiles() function does.\nExercise 2c. Run leaflet(beer_location) |&gt; setView(lng = -98.583, lat = 39.833, zoom = 4) |&gt; addTiles() |&gt; addProviderTiles(providers$Wikimedia) and explain what the addProviderTiles() function does. You may also want to check out the help ?addProviderTiles.\nExercise 2d. Run leaflet(beer_location) |&gt; setView(lng = -98.583, lat = 39.833, zoom = 4) |&gt; addTiles() |&gt; addProviderTiles(providers$Wikimedia) |&gt; addMarkers(lng = beer_location$Longitude, lat = beer_location$Latitude) and explain what the addMarkders() function does.\nExercise 2e. Run the code in Exercise 2d, but add clusterOptions = markerClusterOptions() as an argument to addMarkers(). Explain what adding this argument does to the map.\nExercise 2f. Finally, run the code in Exercise 2e but add popup = content$popup as an argument to addMarkers(). Explain what adding this argument does to the map.\nExercise 3. Examine this link&gt; to look at various “provider” tiles (in the scroll bar on the right-hand side of the page). Choose one to change from Wikimedia and explain how the resulting map looks different."
  },
  {
    "objectID": "11-when-to-use.html#plotly-introduction",
    "href": "11-when-to-use.html#plotly-introduction",
    "title": "13  Introduction to Interactivity",
    "section": "\n13.2 plotly Introduction",
    "text": "13.2 plotly Introduction\nThe plotly R package lets us modify a static (non-interactive) graph to become interactive. This is useful if we don’t have any one particular point that we want to highlight or label but we do want to allow the user to explore data that cannot be easily communicated on the plot. Like leaflet, plotly can be used in other software (Python, Julia, etc.) and can be used to make almost any plot interactive in this way.\nTo use plotly, we need to first install the package and load it (along with tidyverse) into our current R session:\n\n## install.packages(\"plotly\")\nlibrary(plotly)\nlibrary(tidyverse)\n\nRecall the Happy Planet Index data set, where we constructed a scatterplot of Wellbeing vs. carbon Footprint:\n\nhpi_df &lt;- read_csv(here::here(\"data/hpi-tidy.csv\"))\nggplot(data = hpi_df, aes(x = Footprint, y = Wellbeing)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\nIf we were interested in labeling 1 particular country, then we can do so with geom_text() or geom_text_repel(). But, if we want the user to be able to hover over any point in the scatterplot to see the name of the country, we can use plotly.\nFirst, we assign out plot a name so that we can reference it with a plotly function. In this example, we name our plot plot1.\n\nplot1 &lt;- ggplot(data = hpi_df, aes(x = Footprint, y = Wellbeing)) +\n  geom_point() +\n  theme_minimal()\n\nThen, we can use the ggplotly() function on our plot to make it interactive.\n\nggplotly(plot1)\n\n\n\n\n\nSimple! However, by default, plotly labels the points with their x and y values, not by another variable in the data set. We need to add a label aes() to our plot and then use the tooltip argument in ggplotly() to specify that points should be labeled by what we specified in the label aesthetic.\n\nplot1 &lt;- ggplot(data = hpi_df, aes(x = Footprint, y = Wellbeing,\n                                   label = Country)) +\n  geom_point() +\n  theme_minimal()\n\n\nggplotly(plot1, tooltip = \"label\")\n\n\n\n\n\nYour plot should now be interactive and label the points by their country: sick! The syntax for ggplotly() might seem kind of odd now, but, because it’s consistent for all types of plots (specifying a label aesthetic and then using tooltip = \"label\"), it’s actually a pretty simple package to use!\nExercise 1. Use the ggplotly() function on any bar plot that we’ve made so far in the course.\nExercise 2. Use the ggplotly() function on any side-by-side box plots that we’ve made so far in the course.\nExercise 3. Use the ggplotly() function on any faceted plot that we’ve made so far in the course.\nExercise 4. Use the ggplotly() function on any plot of your choice that we’ve made so far in the course. For this exercise, really try to think about a plot that would benefit from becoming interactive.\nExercise 5. You can also remove hover info for particular items with the style() function. In particular, specifying hoverinfo == \"none\" for particular traces removes the hover info for those traces. Modify one of the following examples so that hover info is only displayed for the points (and not for the smoother or the standard error bar for the smoother).\n\nplot_test &lt;- ggplot(hpi_df, aes(x = Footprint, y = Wellbeing)) +\n  geom_point() +\n  geom_smooth()\n\n\nggplotly(plot_test) |&gt;\n  style(hoverinfo = \"none\", traces = c(1, 2, 3)) ## remove hover info for all three traces\n\n\n\n\n\nggplotly(plot_test) |&gt;\n  style(hoverinfo = \"none\", traces = c(3)) ## remove hover info for se line"
  },
  {
    "objectID": "11-when-to-use.html#your-turn",
    "href": "11-when-to-use.html#your-turn",
    "title": "13  Introduction to Interactivity",
    "section": "\n13.3 Your Turn",
    "text": "13.3 Your Turn\nExercise 1. What are some advantages of making a plot more interactive? What are some disadvantages of making a plot more interactive?"
  },
  {
    "objectID": "12-shiny-intro.html#what-is-shiny-class-prep",
    "href": "12-shiny-intro.html#what-is-shiny-class-prep",
    "title": "14  Introduction to Shiny",
    "section": "\n14.1 What is Shiny? (Class Prep)",
    "text": "14.1 What is Shiny? (Class Prep)\nIn this section, we will introduce Shiny, an R package used to make interactive web apps. Unlike other R packages, the shiny package has a more specialized syntax and can take some more work to learn. In particular, the concept of reactivity presents some challenges to coding and debugging that we have not yet seen in this course.\nThe purpose of the following exercises is to give an idea about the types of apps that are possible with shiny.\nExercise 1. Many students and faculty at St. Lawrence have used Shiny for some really cool projects! Some of these are stored at https://stlawu.shinyapps.io/index/. Choose an app from either the tab for SYE projects or the tab for Summer Research projects and answer the following questions.\n\nWhat makes the app interactive? In other words, what is a user allowed to change in the app?\nWhat are the outputs of the app? In other words, what in the app updates when you change some of the things you described in part (a).\nWrite a short one paragraph description describing the app you selected, what makes it interactive, and something interesting that you found while exploring the app.\n\nExercise 2. Choose another app from the SYE or the summer research tab at https://stlawu.shinyapps.io/index/. Answer the following questions.\n\nWhat makes the app interactive? In other words, what is a user allowed to change in the app?\nWhat are the outputs of the app? In other words, what in the app updates when you change some of the things you described in part (a).\n\nNext, to introduce yourself to some basic Shiny code syntax, read Chapter 1 of the Mastering Shiny textbook by Wickham: https://mastering-shiny.org/basic-app.html. Make sure that you can run the code in this section as you are reading along.\nExercise 3. At the end of the section, complete the following exercises in 1.8 Exercises: Exercise 2, Exercise 3, and Exercise 4 (for this exercise, you should actually make the reactive expression that helps reduce the code duplication)."
  },
  {
    "objectID": "12-shiny-intro.html#our-first-app-slu-majors",
    "href": "12-shiny-intro.html#our-first-app-slu-majors",
    "title": "14  Introduction to Shiny",
    "section": "\n14.2 Our First App: SLU Majors",
    "text": "14.2 Our First App: SLU Majors\nWe will use the SLU majors data set to build our very first shiny app. Recall that we made a graph that showed the “other” majors of graduating SLU STAT majors from the past 5 years. However, what if we wanted to allow a user to visually explore this type of graph for any major at SLU?\nTo accomplish this task, we might consider using shiny to build a web app. A usual starting point would be to make the graph for a single major, which we have already done:\n\nlibrary(tidyverse)\nlibrary(readxl)\ntheme_set(theme_minimal())\n\ndf &lt;- read_excel(\"data/slu_graduates_17_23.xlsx\")\n\n## fixes error in the data\ndf &lt;- df |&gt; mutate(across(everything(),\n                           .fns = ~replace(., . ==  \"STATS\" , \"STAT\")))\n\ndf_long &lt;- df |&gt; pivot_longer(4:9, names_to = \"type\",\n                              values_to = \"discipline\")\ndf_major &lt;- df_long |&gt;\n  filter(type == \"major1\" | type == \"major2\" | type == \"major3\")\n\ndf_stat &lt;- df_major |&gt; filter(discipline == \"STAT\") \ndf_statfull &lt;- semi_join(df_long, df_stat, by = \"adm_id\") |&gt;\n  filter(type == \"major1\" |\n           type == \"major2\" | \n           type == \"major3\")\n\ndf_nostat &lt;- df_statfull |&gt; filter(discipline != \"STAT\" &\n                              !is.na(discipline)) |&gt;\n  group_by(discipline) |&gt;\n  summarise(nstudent = n()) |&gt;\n  mutate(discipline = fct_reorder(discipline, nstudent))\nggplot(data = df_nostat, aes(x = discipline, y = nstudent)) +\n  geom_col() +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\nNow, we need to use shiny syntax to make the app. First, install the shiny package:\n\n## install.packages(\"shiny\")\nlibrary(shiny)\n\nA nice shortcut to creating the “bones” of a shiny app is to type shinyapp in an R chunk and clicking on the shinyapp snippet option that pops up. Doing so should create:\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  \n)\n\nserver &lt;- function(input, output, session) {\n  \n}\n\nshinyApp(ui, server)\n\nWe see a couple of components: a ui component, a server function, and a line that will eventually run the app with the shinyApp() function. Go ahead and run each line of code here. After running shinyApp(ui, server), a blank webpage should pull up. This is blank because we haven’t put anything in our shiny app yet!\nOnce you’re done gazing at the blank page, you can return to R Studio and click the red STOP button in the upper-right corner of the console window. Doing so will cause the shiny app to terminate and will allow you to run regular R code again.\nNow that we know a little more about the basic components of shiny, we can start to think about how to build up our app. I find shiny quite challenging to get started with, but the shiny cheatsheet, along with good old StackExchange and StackOverflow can help.\nTo get us started, we will build this SLU majors App as a class. The SLU Majors app will cover a very common case of interaction: allowing the user to choose a value of a variable to filter() by. Note that the course materials site does not support Shiny apps so you will need to run all of the code yourself to view all of the app output.\n\n\n\n\n\n\nImportant\n\n\n\nThe following gives a loose outline of how I would proceed to construct the SLU majors app. Note that, because debugging is more challenging in shiny, I find that making the app in a slow step-by-step fashion to be the best strategy.\n\n\nStep 1: Build a static version of the app you want to create. Typically, to do this, you’ll have to choose particular values for what you want the user of the app to eventually be able to change. We have already completed this step by making the graph for STAT majors.\nStep 2: Decide on and set up an input for the User Interface, UI. We will do this as a class.\nStep 3: Put the static graph (or table) in the server function.\nStep 4: Connect the UI selection input to the server, possibly creating a reactive value. Anything that is reactive must go inside a reactive({}) expression, or a render_({}) expression."
  },
  {
    "objectID": "12-shiny-intro.html#your-turn",
    "href": "12-shiny-intro.html#your-turn",
    "title": "14  Introduction to Shiny",
    "section": "\n14.3 Your Turn",
    "text": "14.3 Your Turn\nExercise 1. Look at the limited choices section of the UI Chapter of Mastering Shiny. In the SLU majors app, change the major input to radioButtons() so that the user can only select mathematics, statistics, computer science, or data science as the major.\nExercise 2. Look at the tables section of the UI Chapter of Mastering Shiny. In the SLU majors app, add an output table with tableOutput() and renderTable() below the plot that shows the number of Female majors and the number of Male majors for a major that the user selects.\nExercise 3. Look at the tables section of the UI Chapter of Mastering Shiny. In the SLU majors app, add a “searchable” data table of the original raw data df with the dataTableOutput() and renderDataTable() functions.\nExercise 4. Add an additional input and output of your choice to the majors app."
  },
  {
    "objectID": "13-shiny-reactivity.html#basic-reactivity-class-prep",
    "href": "13-shiny-reactivity.html#basic-reactivity-class-prep",
    "title": "15  Shiny Reactivity",
    "section": "\n15.1 Basic Reactivity (Class Prep)",
    "text": "15.1 Basic Reactivity (Class Prep)\nRead Sections 3.1 through 3.3 in the Basic Reactivity Chapter of Mastering Shiny\nExercise 1. 3.3.6 Exercise 1 in Mastering Shiny.\nExercise 2. 3.3.6 Exercise 2 in Mastering Shiny.\nIn Wickham’s Mastering Shiny book, there are a couple of sections on reactive graphs, which help conceptualize how shiny runs code. Read Section 3.4: the Basic Reactivity Chapter of Mastering Shiny, and complete the following.\nExercise 3. Sketch a reactive graph for the majors app in the previous section. Make sure to include all inputs, outputs, and reactive expressions in your graph. Instead of submitting this exercise, bring your sketch to class."
  },
  {
    "objectID": "13-shiny-reactivity.html#another-example-tennis-app",
    "href": "13-shiny-reactivity.html#another-example-tennis-app",
    "title": "15  Shiny Reactivity",
    "section": "\n15.2 Another Example Tennis App",
    "text": "15.2 Another Example Tennis App\nThe purpose of this section is to complete another example with a different data set, continuing to strengthen our conceptual understanding of reactivity. We will use a couple of tennis data sets obtained from https://github.com/JeffSackmann/tennis_atp and https://github.com/JeffSackmann/tennis_wta to create an app that lets us create a histogram of a summary statistic of our choosing for a player of our choosing. Constructing this app will cover another common use case: allowing a user to choose a variable from a data set to plot.\nAgain, a usual first step is creating a static graph, choosing a value for each future interactive input. Begin by reading in the data and doing a bit of preparatory work. To focus on shiny, we will skip discussion of this work, but all of it are functions you should recognize.\n\nlibrary(tidyverse)\n\ntheme_set(theme_minimal())\n\natp_df &lt;- read_csv(\"https://raw.githubusercontent.com/JeffSackmann/tennis_atp/master/atp_matches_2023.csv\")\nwta_df &lt;- read_csv(\"https://raw.githubusercontent.com/JeffSackmann/tennis_wta/master/wta_matches_2023.csv\")\n\nboth_df &lt;- bind_rows(atp_df, wta_df)\n\nboth_long &lt;- both_df |&gt; pivot_longer(c(winner_name, loser_name))\n\n## only keep players who have player over 50 matches\nboth_n50 &lt;- both_long |&gt; group_by(value) |&gt; count() |&gt;\n  filter(n &gt; 50)\n\n## construct various statistics\nmajor_tennis &lt;- semi_join(both_long, both_n50, by = c(\"value\"))\nmajor_tennis &lt;- major_tennis |&gt; mutate(w_svperc = 100 * w_1stIn / w_svpt,\n                        l_svperc = 100 * l_1stIn / l_svpt,\n                        w_firstwon = 100 * w_1stWon / w_1stIn,\n                        l_firstwon = 100 * l_1stWon / l_1stIn,\n                        w_secondwon = 100 * w_2ndWon / (w_svpt - w_1stIn),\n                        l_secondwon = 100 * l_2ndWon / (l_svpt - l_1stIn))\n\nThe next chunk of code is needed to put the various statistics into one column. This is an issue because, in major_tennis, there is a column for w_ace (aces for the winner) and l_ace (aces for the loser). We need those in one column, aces, that have the number of aces for the player. Looking back, this is not the most efficient way we could do this. If we were to do it again, we could do an across() combined with if_else() where the new variables would take the w_.... values if the player was the winner and the l_.... values if the player was the loser.\n\nmajor_tennis_w &lt;- major_tennis |&gt; filter(name == \"winner_name\")\nmajor_tennis_l &lt;- major_tennis |&gt; filter(name == \"loser_name\")\n\nw_small &lt;- major_tennis_w |&gt; select(value, winner_seed, w_ace, w_df,\n                                    w_svperc, w_firstwon, w_secondwon) |&gt;\n  rename(seed = winner_seed, ace = w_ace, df = w_df, svperc = w_svperc,\n         firstwon = w_firstwon, secondwon = w_secondwon)\n\nl_small &lt;- major_tennis_l |&gt; select(value, loser_seed, l_ace, l_df,\n                                    l_svperc, l_firstwon, l_secondwon) |&gt;\n  rename(seed = loser_seed, ace = l_ace, df = l_df, svperc = l_svperc,\n         firstwon = l_firstwon, secondwon = l_secondwon)\n\ndf &lt;- bind_rows(w_small, l_small) |&gt;\n  rename(player = \"value\")\ndf\n#&gt; # A tibble: 3,637 × 7\n#&gt;   player              seed   ace    df svperc firstwon secondwon\n#&gt;   &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 Taylor Fritz           3    15     2   61.2     86.5      48.5\n#&gt; 2 Frances Tiafoe        NA     3     1   70.4     78.9      37.5\n#&gt; 3 Taylor Fritz           3    11     0   62.5     88        63.3\n#&gt; 4 Frances Tiafoe        NA     5     4   60.7     83.8      45.8\n#&gt; 5 Stefanos Tsitsipas     1     9     1   53.1     82.7      60.9\n#&gt; 6 Lorenzo Musetti       NA     2     1   61.0     72        75  \n#&gt; # ℹ 3,631 more rows\n\nNow, we have our five variables (ace, df, svperc, firstwon, and secondwon). We want an app that lets that user make a histogram of one of these variables for a player of their choosing.\nWe will try to use a similar workflow for this problem as we did for the majors app.\nStep 1: Make a histogram of one variable for one specific player.\nStep 2: Set up our shiny app inputs. Before, we just had a single input. Now, we will have two: one for player and one for variable. Let’s focus on one at a time, doing player first. Type shinyapp and click on the R Studio snippet to bring up a base app.\nStep 3: Now that we have one of our inputs in the UI, let’s work on the server. First, we will ignore the input$ selector and put in our graph of aces for Medvedev. We again use the plotOutput(), renderPlot({}) combination.\nStep 4: Now we want to connect the input defined in the UI to the server so that the graph changes depending on which player we select.\nStep 5: Now we repeat some of these steps for a second input: a variable that the user selects. We will use radioButtons() as the input in the UI.\nWe will discuss why we need to take some extra steps to perform the “user can select a variable” operation in class. In particular, we will need to briefly discuss tidy evaluation to use some of the tidyverse functions in shiny.\nStep 6: Finally, we will add a third input that will let the user change the number of bins in the histogram.\nExercise 1. If we move the output$name_of_histogram &lt;- .... code to the very beginning of the server functino, do you think the app will still run? Give a reason.\nExercise 2. Draw the reactive graph for the Tennis app."
  },
  {
    "objectID": "13-shiny-reactivity.html#your-turn",
    "href": "13-shiny-reactivity.html#your-turn",
    "title": "15  Shiny Reactivity",
    "section": "\n15.3 Your Turn",
    "text": "15.3 Your Turn\nExercise 1. In the tennis app, change the histogram input from a sliderInput() to a numericInput().\nExercise 2. In the tennis app, change the histogram input back to sliderinput(). Then, apply 2.2.8 Exercise 3 in the Mastering Shiny book to the tennis app slider.\nExercise 3. Add an additional input or output (or both!) to the Tennis App. You can decide what you want to add here!"
  },
  {
    "objectID": "13-shiny-reactivity.html#other-useful-things-for-shiny",
    "href": "13-shiny-reactivity.html#other-useful-things-for-shiny",
    "title": "15  Shiny Reactivity",
    "section": "\n15.4 Other Useful Things for shiny\n",
    "text": "15.4 Other Useful Things for shiny\n\n\n15.4.1 glue::glue()\n\nThe glue() function from the glue package is really useful for making plot titles, axis titles, etc. that depend on variables you have made. For example, you might want a title for ggplot2 to change depending on which input a user selects.\nIt’s a fairly simple function: you just put together text in \" \" and variable names, separated by commas.\n\npoke_df &lt;- read_csv(here::here(\"data/pokemon_full.csv\"))\npoke_long &lt;- poke_df %&gt;% pivot_longer(4:9, values_to = \"value\", \n                                      names_to = \"stat\")\n\npoke_small &lt;- poke_long %&gt;% filter(Name == \"Bulbasaur\" | Name == \"Ivysaur\")\nggplot(data = poke_small, aes(x = stat, y = value)) +\n  geom_col(aes(fill = Name), position = \"dodge\")\n\n\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      selectizeInput(\"pokechoose1\", label = \"Choose Pokemon\",\n                     choices = poke_df$Name, selected = \"Bulbasaur\")\n    ),\n    mainPanel(\n      plotOutput(outputId = \"pokegraph\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  output$pokegraph &lt;- renderPlot({\n    poke_react &lt;- poke_long %&gt;% filter(Name %in% input$pokechoose1)\n    \n    ggplot(data = poke_react, aes(x = stat, y = value)) +\n      geom_point() +\n      geom_segment(aes(xend = stat, y = 0, yend = value)) +\n      coord_flip() +\n      labs(title = glue::glue(\"Graph of the Stats for\", input$pokechoose1))\n  })\n  \n}\n\nshinyApp(ui, server)\n\n\n\n\n\n15.4.2 Action Button\nAn action button is useful if you do not want your plot to update until the user clicks the button. There are a couple of useful cases for an action button:\n\nthe code in your Shiny app takes a long time to run (so you don’t want it updating each time the user changes an input).\nthe output does not make sense unless the user changes multiple inputs (so again, you don’t want the shiny app to run each time the user changes an input).\n\nTake a look at this example for the Pokemon app:\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      selectizeInput(\"pokechoose1\", label = \"Choose Pokemon\",\n                     choices = poke_df$Name, selected = \"Bulbasaur\",\n                     multiple = TRUE),\n      actionButton(\"runappbutton\", label = \"Update Stats\")\n    ),\n    mainPanel(\n      plotOutput(outputId = \"pokegraph\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  poke_react &lt;-  eventReactive(input$runappbutton, {\n    poke_long |&gt; filter(Name %in% input$pokechoose1)\n  })\n  \n  output$pokegraph &lt;- renderPlot({\n    \n    ggplot(data = poke_react(), aes(x = stat, y = value)) +\n      geom_col(aes(fill = Name), position = \"dodge\") +\n      coord_flip() +\n      scale_fill_viridis_d()\n  }\n  )\n  \n}\n\nshinyApp(ui, server)\n\n\n\n\nYou must often pair an actionButton with either eventReactive() or observeEvent(). The differences between the two are a little confusing but the primary difference is that eventReactive() is used to create a reactive expression when the action button is clicked while observeEvent() just runs some code when an action button is clicked.\n\n15.4.3 Using if and else\n\nIf we want our app do do something completely different when a user changes an input, we might consider using if and else. If we have time in class, we will do an example in Shiny: if an input is equal to some value, do something or make a certain plot; else make a different type of plot. Below is an example of the syntax in base R:\n\nx &lt;- 5\nif (x &lt; 10) {\n  x * 2\n} else {\n  x / 2\n}\n#&gt; [1] 10\n\n\n15.4.4 source() and write_csv()\n\n\n\n\n\n\n\nImportant\n\n\n\nFor shiny apps, if there is a lot of data preparation before the data is ready for the shiny app, it is generally best to put this data preparation in a different R script.\n\n\nYou can then either source() the data preparation file (which runs all of the code in the file that is sourced) or you can use write_csv() to write the prepped data to its own .csv file. You would then use read_csv() on this new file in the app.R file with your shiny app."
  }
]